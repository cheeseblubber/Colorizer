{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62340f-265b-4a2a-8032-c4d78cbbe551",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (anything_vae.py, line 199)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/dl-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\n\u001b[0;31m    from anything_vae import (\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/workspace/anything_vae.py:199\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1, stride=)\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from anything_vae import (\n",
    "    ResnetBlock2D,\n",
    "    SelfAttention,\n",
    "    Downsample2D,\n",
    "    Upsample2D,\n",
    "    DownEncoderBlock2D,\n",
    "    UpDecoderBlock2D,\n",
    "    UNetMidBlock2D,\n",
    "    Encoder,\n",
    "    Decoder,\n",
    "    # AutoencoderKL,\n",
    "    VGGPerceptualLoss\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models as torchvision_models\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers, callbacks\n",
    "# import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fa544-ca50-4173-8ac0-00d5f5023f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    # data\n",
    "    def __init__(self, data_folder, data_csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dir (string): Directory with all the input images.\n",
    "            output_dir (string): Directory with all the target (color) images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.data_path = os.path.join(data_folder, data_csv)\n",
    "        self.images = pd.read_csv(self.data_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB by replicating channels\n",
    "            transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "        ])\n",
    "        self.tranform_output = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10\n",
    "        # return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        sketch_image = self.transform(self.__loadImage(sketch))\n",
    "        colored_image = self.tranform_output(self.__loadImage(colored))\n",
    "        return sketch_image, colored_image\n",
    "\n",
    "    def viewImage(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        return self.__loadImage(sketch), self.__loadImage(colored)\n",
    "\n",
    "    def __loadImage(self, image_path):\n",
    "        return Image.open(os.path.join(self.data_folder, image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c999b1-0fc7-438d-9062-0c945538f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VGGPerceptualLoss(LightningModule):\n",
    "#     def __init__(self, vgg_model):\n",
    "#         super().__init__()\n",
    "#         self.vgg = vgg_model\n",
    "#         self.criterion = nn.MSELoss()\n",
    "#         self.features = list(self.vgg.features[:16])\n",
    "#         self.features = nn.Sequential(*self.features).eval()\n",
    "        \n",
    "#         for params in self.features.parameters():\n",
    "#             params.requires_grad = False\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         return self.criterion(self.features(x),self.features(y))\n",
    "\n",
    "# class Colorizer(LightningModule):\n",
    "#     def __init__(self, vae):\n",
    "#         super().__init__()\n",
    "#         self.model = vae\n",
    "#         vgg_model = vgg16(weights=True)\n",
    "#         self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "#         self.mse_loss_fn = nn.MSELoss()  # MSE Loss function\n",
    "\n",
    "        \n",
    "#         # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         # self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "#         # self.clip_loss_fn = CLIPLoss(self.clip_model, \"vibrant beautiful colorful\", device)\n",
    "# # \n",
    "#         self.hparams.learning_rate = 0.00001\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.hparams.learning_rate)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         inputs, targets = batch\n",
    "#         outputs = self(inputs).sample\n",
    "\n",
    "#         perceptual_loss = self.loss_fn(outputs, targets)\n",
    "#         # clip_loss = self.clip_loss_fn(outputs)\n",
    "#         mse_loss = self.mse_loss_fn(outputs, targets)  # Compute MSE loss\n",
    "\n",
    "#         # total_loss = perceptual_loss + clip_loss\n",
    "#         total_loss = perceptual_loss + mse_loss\n",
    "#         # total_loss = perceptual_loss\n",
    "#         self.log('train_loss', total_loss)\n",
    "#         self.log('perceptual_loss', perceptual_loss)\n",
    "#         self.log('mse_loss', mse_loss)\n",
    "#         return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371c0b5-5b32-4103-a6a2-c325760c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)  # Output 4 channels from quant_conv\n",
    "        self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)  # Expect 4 channels here\n",
    "\n",
    "        self.loss_fn = VGGPerceptualLoss()\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)  # h: [batch_size, 8, H, W]\n",
    "\n",
    "        # Split h into mean and logvar\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)  # Each: [batch_size, 4, H, W]\n",
    "\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + std * eps  # z: [batch_size, 4, H, W]\n",
    "\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.post_quant_conv(z)  # z: [batch_size, 4, H, W]\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        print(f\"Input shape: {x.shape}, Reconstructed shape: {x_recon.shape}\")\n",
    "        \n",
    "        # Compute losses\n",
    "        perceptual_loss = self.loss_fn(x_recon, x)\n",
    "        mse_loss = self.mse_loss_fn(x_recon, x)\n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        \n",
    "        return x_recon, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c791a-ea56-4bef-b7c3-c54771c55e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ColorizerOLD(nn.Module):\n",
    "#     def __init__(self, pretrained_vae):\n",
    "#         super(ColorizerOLD, self).__init__()\n",
    "\n",
    "#         # Use pretrained encoder, decoder, quant_conv, and post_quant_conv\n",
    "#         self.encoder = pretrained_vae.encoder\n",
    "#         self.decoder = pretrained_vae.decoder\n",
    "#         self.quant_conv = pretrained_vae.quant_conv\n",
    "#         self.post_quant_conv = pretrained_vae.post_quant_conv\n",
    "\n",
    "#         vgg_model = vgg16(weights=True)\n",
    "\n",
    "#         self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "#         self.mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         h = self.quant_conv(h)  # h: [batch_size, 8, H, W]\n",
    "\n",
    "#         # Split h into mean and logvar\n",
    "#         mean, logvar = torch.chunk(h, 2, dim=1)  # Each: [batch_size, 4, H, W]\n",
    "\n",
    "#         # Reparameterization trick\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         z = mean + std * eps  # z: [batch_size, 4, H, W]\n",
    "\n",
    "#         return z\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         z = self.post_quant_conv(z)  # z: [batch_size, 4, H, W]\n",
    "#         x_recon = self.decoder(z)\n",
    "#         return x_recon\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         z = self.encode(x)\n",
    "#         x_recon = self.decode(z)\n",
    "\n",
    "#         # Compute losses\n",
    "#         perceptual_loss = self.loss_fn(x_recon, x)\n",
    "#         mse_loss = self.mse_loss_fn(x_recon, x)\n",
    "#         total_loss = perceptual_loss + mse_loss\n",
    "\n",
    "#         return x_recon, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02695f08-a0c1-43ae-a938-fd23dbc33fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, dataset, device, batch_size=1, lr=1e-4, epochs=10):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.model.train()\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for i, (sketch, colored) in enumerate(self.dataloader):\n",
    "                sketch, colored = sketch.to(self.device), colored.to(self.device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                reconstructed, loss = self.model(sketch)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Optimize\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Print average loss for the epoch\n",
    "            avg_loss = total_loss / len(self.dataloader)\n",
    "            print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7868e-3ebf-4ee5-bc0b-86337f4d646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = torch.load('anything-vae.pth', map_location='cpu')\n",
    "# model = Colorizer(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfda57-a0cd-46df-bef5-c62e77c4079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Colorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff49e4-8c48-44a2-a38a-4e4df10013ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "\n",
    "# Load the state dict into your model, but ignore missing loss_fn keys\n",
    "missing_keys, unexpected_keys = model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "\n",
    "# Filter out the missing loss_fn keys, as they are not critical\n",
    "filtered_missing_keys = [key for key in missing_keys if not key.startswith('loss_fn')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafbecd-e11f-42e4-ae6b-0c83c94d7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_missing_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c37887-c6a7-49f9-b029-e0797cc693de",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(filtered_missing_keys) == 0\n",
    "assert len(unexpected_keys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dc6bb-a4d1-403a-93b7-78cb980697d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/toy'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9efaf-17f4-4895-819f-d2ae21cd618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = loggers.TensorBoardLogger(\"tb_logs\")\n",
    "# trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=30, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce4e5-a70b-4e74-ab36-3fa834cbb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f6991-ed37-4c6b-92bb-da9548e614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_dataset, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea0e0f-a489-4c42-a9ac-ceadcde4c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d19358-87aa-42af-bcc4-87082b93284f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be137035-d800-49d2-aa32-a1b56fcc6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def viewTensor(output):\n",
    "    image = to_pil_image(output.squeeze())\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af58b2-33bb-4b65-8644-48112b2b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data_folder = 'data/test'\n",
    "data_csv = 'data.csv'\n",
    "test_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabfd6-8430-4466-a3e4-a707b7cac420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "x, y = training_dataset[idx]\n",
    "output = model(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd70dc-50de-4bcf-8f95-8a4c0503ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1268b-3ee2-4167-a450-0068e6bfa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18232e0-e302-44e3-8ca9-6d59d02e91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a514-33e9-42c7-989d-96397ed4d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
