{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62340f-265b-4a2a-8032-c4d78cbbe551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from anything_vae import (\n",
    "    Encoder,\n",
    "    Decoder,\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models as torchvision_models\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers, callbacks\n",
    "# import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501fa544-ca50-4173-8ac0-00d5f5023f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    # data\n",
    "    def __init__(self, data_folder, data_csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dir (string): Directory with all the input images.\n",
    "            output_dir (string): Directory with all the target (color) images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.data_path = os.path.join(data_folder, data_csv)\n",
    "        self.images = pd.read_csv(self.data_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB by replicating channels\n",
    "            transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "        ])\n",
    "        self.tranform_output = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "        # return len(self.images)/\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        sketch_image = self.transform(self.__loadImage(sketch))\n",
    "        colored_image = self.tranform_output(self.__loadImage(colored))\n",
    "        return sketch_image, colored_image\n",
    "\n",
    "    def viewImage(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        return self.__loadImage(sketch), self.__loadImage(colored)\n",
    "\n",
    "    def __loadImage(self, image_path):\n",
    "        return Image.open(os.path.join(self.data_folder, image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c999b1-0fc7-438d-9062-0c945538f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(LightningModule):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg_model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.features = list(self.vgg.features[:16])\n",
    "        self.features = nn.Sequential(*self.features).eval()\n",
    "        \n",
    "        for params in self.features.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.criterion(self.features(x),self.features(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a371c0b5-5b32-4103-a6a2-c325760c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Conv2d(dim, dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(lambda t: t.reshape(b, self.heads, -1, h * w), qkv)\n",
    "\n",
    "        dots = torch.matmul(q.transpose(-2, -1), k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = torch.matmul(attn, v.transpose(-2, -1))\n",
    "        out = out.reshape(b, -1, h, w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(dim, heads)\n",
    "        # Change normalization to handle spatial dimensions\n",
    "        self.norm1 = nn.GroupNorm(8, dim)  # Using GroupNorm instead of LayerNorm\n",
    "        self.norm2 = nn.GroupNorm(8, dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim * 4, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(dim * 4, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Colorizer(LightningModule):\n",
    "    def __init__(self, checkpoint_path=None, transformer_dim=256, transformer_heads=8):\n",
    "        super(Colorizer, self).__init__()\n",
    "        if checkpoint_path is not None:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            \n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            \n",
    "            self.encoder.load_state_dict(\n",
    "                {k.replace('encoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('encoder.')}\n",
    "            )\n",
    "            self.decoder.load_state_dict(\n",
    "                {k.replace('decoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('decoder.')}\n",
    "            )\n",
    "            self.quant_conv.load_state_dict(\n",
    "                {k.replace('quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('quant_conv.')}\n",
    "            )\n",
    "            self.post_quant_conv.load_state_dict(\n",
    "                {k.replace('post_quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('post_quant_conv.')}\n",
    "            )\n",
    "            \n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            \n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.post_quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "            print(\"Loaded pretrained weights from checkpoint\")\n",
    "        else:\n",
    "            # Initialize new components\n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            print(\"Initialized new model from scratch\")\n",
    "\n",
    "            \n",
    "        # Add transformer layers (always initialized from scratch)\n",
    "        self.to_transformer_dim = nn.Conv2d(4, transformer_dim, 1)\n",
    "        self.transformer = TransformerBlock(transformer_dim, transformer_heads)\n",
    "        self.from_transformer_dim = nn.Conv2d(transformer_dim, 4, 1)\n",
    "        self.num_high_loss_images = 50\n",
    "        self.high_loss_heap = []  # Min heap to track top N highest loss images\n",
    "        self.current_min_high_loss = 0  # Current minimum loss in our high loss collection\n",
    "        \n",
    "        self.hparams.learning_rate = 0.0001\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + std * eps\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Apply transformer before decoding\n",
    "        z = self.post_quant_conv(z)\n",
    "        z = self.to_transformer_dim(z)\n",
    "        z = self.transformer(z)\n",
    "        z = self.from_transformer_dim(z)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()), \n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "\n",
    "    def store_high_loss_image(self, loss, inputs, targets, outputs):\n",
    "        \"\"\"\n",
    "        Store high loss images efficiently using a min heap to maintain top N\n",
    "        All tensors are stored in CPU memory\n",
    "        \"\"\"\n",
    "        # Convert to CPU and detach from computation graph\n",
    "        cpu_data = {\n",
    "            'loss': loss.item(),\n",
    "            'inputs': inputs.detach().cpu(),\n",
    "            'targets': targets.detach().cpu(),\n",
    "            'outputs': outputs.detach().cpu()\n",
    "        }\n",
    "        \n",
    "        if len(self.high_loss_heap) < self.num_high_loss_images:\n",
    "            # Heap not full yet, add new entry\n",
    "            heapq.heappush(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = min(loss.item(), self.current_min_high_loss if self.high_loss_heap else float('inf'))\n",
    "        elif loss.item() > self.current_min_high_loss:\n",
    "            # Remove lowest loss entry and add new higher loss entry\n",
    "            heapq.heapreplace(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = self.high_loss_heap[0][0]\n",
    "\n",
    "\n",
    "    def visualize_model_output(self, inputs, outputs, targets, logger, step, tag):\n",
    "        \"\"\"Helper function to create and log visualization grid with horizontal layout\"\"\"\n",
    "        inputs = inputs.detach().cpu()\n",
    "        outputs = outputs.detach().cpu()\n",
    "        targets = targets.detach().cpu()\n",
    "        \n",
    "        n = inputs.shape[0]\n",
    "        \n",
    "        grid = []\n",
    "        for i in range(n):\n",
    "            row = torch.stack([inputs[i], outputs[i], targets[i]])\n",
    "            grid.append(row)\n",
    "        \n",
    "        grid = torch.cat(grid, dim=0)\n",
    "        \n",
    "        if grid.shape[1] == 1:\n",
    "            grid = grid.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        grid = torchvision.utils.make_grid(grid, nrow=3, normalize=True, padding=2)\n",
    "        \n",
    "        logger.experiment.add_image(tag, grid, step)\n",
    "\n",
    "    def visualize_single_output(self, input_img, output_img, target_img):\n",
    "        \"\"\"Helper function to create a single row grid for one set of images\"\"\"\n",
    "        # Ensure we're working with batched images\n",
    "        if input_img.dim() == 3:\n",
    "            input_img = input_img.unsqueeze(0)\n",
    "            output_img = output_img.unsqueeze(0)\n",
    "            target_img = target_img.unsqueeze(0)\n",
    "            \n",
    "        # Create row with input, output, and target\n",
    "        row = torch.cat([input_img, output_img, target_img], dim=0)\n",
    "        \n",
    "        # Handle grayscale images\n",
    "        if row.shape[1] == 1:\n",
    "            row = row.repeat(1, 3, 1, 1)\n",
    "            \n",
    "        # Create grid with the three images side by side\n",
    "        grid = torchvision.utils.make_grid(row, nrow=3, normalize=True, padding=2)\n",
    "        return grid\n",
    "\n",
    "    def visualize_high_loss_images(self, logger, step):\n",
    "        \"\"\"Visualize stored high loss images individually\"\"\"\n",
    "        if not self.high_loss_heap:\n",
    "            return\n",
    "            \n",
    "        # Sort by loss in descending order\n",
    "        sorted_entries = sorted(self.high_loss_heap, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Log each high-loss image separately\n",
    "        for idx, (loss_value, data) in enumerate(sorted_entries):\n",
    "            grid = self.visualize_single_output(\n",
    "                data['inputs'],\n",
    "                data['outputs'],\n",
    "                data['targets']\n",
    "            )\n",
    "\n",
    "            logger.experiment.add_image('High_Loss_Images', grid, step)\n",
    "                    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "    \n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "        mse_loss = self.mse_loss_fn(outputs, targets)\n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        \n",
    "        # Basic loss logging\n",
    "        self.log('train_loss', total_loss)\n",
    "        self.log('perceptual_loss', perceptual_loss)\n",
    "        self.log('mse_loss', mse_loss)\n",
    "        \n",
    "        # Store high loss images\n",
    "        self.store_high_loss_image(total_loss, inputs, targets, outputs)\n",
    "        \n",
    "        # Every 100 batches, visualize the current high loss images\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            self.visualize_high_loss_images(self.logger, self.global_step)\n",
    "        \n",
    "        # Log sample images every 500 batches\n",
    "        if batch_idx % 500 == 0:\n",
    "            num_images = min(4, inputs.shape[0])\n",
    "            for i in range(num_images):\n",
    "                grid = self.visualize_single_output(\n",
    "                    inputs[i],\n",
    "                    outputs[i],\n",
    "                    targets[i]\n",
    "                )\n",
    "                logger.experiment.add_image(\n",
    "                    f'Sample_Images/sample_{i+1}',\n",
    "                    grid,\n",
    "                    self.global_step\n",
    "                )\n",
    "        \n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b02076-c894-4cf9-bc6e-9199a07cee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights from checkpoint\n"
     ]
    }
   ],
   "source": [
    "chkpt_file = 'checkpoints/version_13.ckpt'\n",
    "model = Colorizer(checkpoint_path=chkpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ff49e4-8c48-44a2-a38a-4e4df10013ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrained_model = torch.load('anything-vae.pth', map_location='cpu')\n",
    "# model = Colorizer()\n",
    "# pretrained_state_dict = pretrained_model.state_dict()\n",
    "# missing_keys, unexpected_keys = model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "# filtered_missing_keys = [key for key in missing_keys if not key.startswith('loss_fn')]\n",
    "# assert len(filtered_missing_keys) == 0\n",
    "# assert len(unexpected_keys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63dc6bb-a4d1-403a-93b7-78cb980697d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/toy'\n",
    "# data_folder = 'data/training'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f9efaf-17f4-4895-819f-d2ae21cd618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = loggers.TensorBoardLogger(\"tb_logs\", name='frozen-pretrained-vae-with-new-transformer')\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=20, logger=logger, log_every_n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce4e5-a70b-4e74-ab36-3fa834cbb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                 | Type              | Params\n",
      "-----------------------------------------------------------\n",
      "0 | encoder              | Encoder           | 34.2 M\n",
      "1 | decoder              | Decoder           | 49.5 M\n",
      "2 | quant_conv           | Conv2d            | 72    \n",
      "3 | post_quant_conv      | Conv2d            | 20    \n",
      "4 | loss_fn              | VGGPerceptualLoss | 138 M \n",
      "5 | mse_loss_fn          | MSELoss           | 0     \n",
      "6 | to_transformer_dim   | Conv2d            | 1.3 K \n",
      "7 | transformer          | TransformerBlock  | 788 K \n",
      "8 | from_transformer_dim | Conv2d            | 1.0 K \n",
      "-----------------------------------------------------------\n",
      "137 M     Trainable params\n",
      "85.4 M    Non-trainable params\n",
      "222 M     Total params\n",
      "891.211   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  48%|████▊     | 479/1000 [04:35<04:59,  1.74it/s, v_num=16] "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f6991-ed37-4c6b-92bb-da9548e614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, training_dataset, device)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be137035-d800-49d2-aa32-a1b56fcc6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def viewTensor(output):\n",
    "    image = to_pil_image(output.squeeze())\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af58b2-33bb-4b65-8644-48112b2b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data_folder = 'data/test'\n",
    "data_csv = 'data.csv'\n",
    "test_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabfd6-8430-4466-a3e4-a707b7cac420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "x, y = test_dataset[idx]\n",
    "output = model(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd70dc-50de-4bcf-8f95-8a4c0503ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1268b-3ee2-4167-a450-0068e6bfa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18232e0-e302-44e3-8ca9-6d59d02e91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a514-33e9-42c7-989d-96397ed4d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
