{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f62340f-265b-4a2a-8032-c4d78cbbe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anything_vae import (\n",
    "    Encoder,\n",
    "    Decoder,\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models as torchvision_models\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers, callbacks\n",
    "# import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import heapq\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "import torchvision.transforms as transforms\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224d38d-a74f-406b-8d3a-9f9224327226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501fa544-ca50-4173-8ac0-00d5f5023f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dominant_colors(image, num_colors=100, num_samples=10000):\n",
    "    \"\"\"\n",
    "    Extract dominant colors from an image using random sampling.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): Input image\n",
    "        num_colors (int): Number of colors to extract\n",
    "        num_samples (int): Number of pixels to sample\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (num_colors, 3) containing RGB values\n",
    "    \"\"\"\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    \n",
    "    # Reshape the array to 2D (pixel_count, 3)\n",
    "    pixels = img_array.reshape(-1, 3)\n",
    "    \n",
    "    # Random sampling\n",
    "    total_pixels = pixels.shape[0]\n",
    "    if total_pixels > num_samples:\n",
    "        indices = np.random.choice(total_pixels, num_samples, replace=False)\n",
    "        pixels = pixels[indices]\n",
    "    \n",
    "    # Round colors to reduce similar shades\n",
    "    pixels = np.round(pixels / 255 * 32) * (255 / 32)\n",
    "    \n",
    "    # Get unique colors and their counts\n",
    "    unique_colors, counts = np.unique(pixels, axis=0, return_counts=True)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_indices = np.argsort(-counts)\n",
    "    sorted_colors = unique_colors[sorted_indices]\n",
    "    \n",
    "    # Take top num_colors\n",
    "    top_colors = sorted_colors[:num_colors]\n",
    "    \n",
    "    # Pad if we don't have enough unique colors\n",
    "    if len(top_colors) < num_colors:\n",
    "        padding = np.tile(top_colors[-1], (num_colors - len(top_colors), 1))\n",
    "        top_colors = np.vstack((top_colors, padding))\n",
    "    \n",
    "    # Convert to tensor and normalize to [0, 1]\n",
    "    color_tensor = torch.FloatTensor(top_colors) / 255.0\n",
    "    \n",
    "    return color_tensor\n",
    "\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, data_folder, data_csv, cache_dir='color_hints_cache', transform=None, hint_offset=3, num_colors=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_folder (string): Directory with all the images.\n",
    "            data_csv (string): CSV file with image paths.\n",
    "            cache_dir (string): Directory to store color hints cache.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            hint_offset (int): Number of images away to fetch the hint image.\n",
    "            num_colors (int): Number of dominant colors to extract from each image.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.data_path = os.path.join(data_folder, data_csv)\n",
    "        self.images = pd.read_csv(self.data_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.tranform_output = transforms.Compose([transforms.ToTensor()])\n",
    "        self.hint_offset = hint_offset\n",
    "        self.num_colors = num_colors\n",
    "        \n",
    "        # Setup cache directory and file\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.cache_file = os.path.join(cache_dir, f'color_hints_{num_colors}_colors.json')\n",
    "        \n",
    "        # Load existing cache or create new one\n",
    "        self.color_hints_cache = self._load_cache()\n",
    "        self.cache_updates = 0  # Counter for tracking updates\n",
    "        \n",
    "        # Extract show names from the file paths\n",
    "        self.images['show'] = self.images['Sketch Path'].apply(\n",
    "            lambda x: os.path.basename(os.path.dirname(x))\n",
    "        )\n",
    "        # Sort the DataFrame by show to group images from the same show\n",
    "        self.images = self.images.sort_values(by=['show']).reset_index(drop=True)\n",
    "\n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load color hints cache from disk\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, 'r') as f:\n",
    "                try:\n",
    "                    cache_dict = json.load(f)\n",
    "                    # Convert stored lists back to tensors\n",
    "                    return {k: torch.tensor(v, dtype=torch.float32) for k, v in cache_dict.items()}\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Cache file {self.cache_file} is corrupted. Creating new cache.\")\n",
    "                    return {}\n",
    "        return {}\n",
    "\n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save color hints cache to disk\"\"\"\n",
    "        # Convert tensors to lists for JSON serialization\n",
    "        cache_dict = {k: v.tolist() for k, v in self.color_hints_cache.items()}\n",
    "        temp_file = self.cache_file + '.tmp'\n",
    "        try:\n",
    "            with open(temp_file, 'w') as f:\n",
    "                json.dump(cache_dict, f)\n",
    "            os.replace(temp_file, self.cache_file)  # Atomic write\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving cache: {str(e)}\")\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "\n",
    "    def _get_or_extract_color_hints(self, colored_path):\n",
    "        \"\"\"Get color hints from cache or extract them\"\"\"\n",
    "        if colored_path not in self.color_hints_cache:\n",
    "            colored_pil = self.__loadImage(colored_path)\n",
    "            color_hints = extract_dominant_colors(colored_pil, self.num_colors)\n",
    "            self.color_hints_cache[colored_path] = color_hints\n",
    "            \n",
    "            # Save to disk every 1000 new extractions\n",
    "            self.cache_updates += 1\n",
    "            if self.cache_updates % 1000 == 0:\n",
    "                print(f\"Saving cache after {self.cache_updates} updates...\")\n",
    "                self._save_cache()\n",
    "                \n",
    "        return self.color_hints_cache[colored_path]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.images.iloc[idx]\n",
    "        sketch = row['Sketch Path']\n",
    "        colored = row['Frame Path']\n",
    "        show = row['show']\n",
    "        \n",
    "        # Get indices of all images from the same show\n",
    "        show_indices = self.images.index[self.images['show'] == show].tolist()\n",
    "        pos_in_show = show_indices.index(idx)\n",
    "        hint_pos_in_show = pos_in_show + self.hint_offset\n",
    "        hint_pos_in_show = max(0, min(hint_pos_in_show, len(show_indices) - 1))\n",
    "        hint_idx = show_indices[hint_pos_in_show]\n",
    "        hint_row = self.images.iloc[hint_idx]\n",
    "        hint_sketch = hint_row['Sketch Path']\n",
    "        hint_colored = hint_row['Frame Path']\n",
    "        \n",
    "        # Load images\n",
    "        sketch_image = self.transform(self.__loadImage(sketch))\n",
    "        colored_image = self.tranform_output(self.__loadImage(colored))\n",
    "        hint_image = self.tranform_output(self.__loadImage(hint_colored))\n",
    "        \n",
    "        # Get color hints using cache\n",
    "        color_hints = self._get_or_extract_color_hints(colored)\n",
    "            \n",
    "        return sketch_image, colored_image, color_hints, hint_image\n",
    "\n",
    "    def viewImage(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx][['Sketch Path', 'Frame Path']]\n",
    "        return self.__loadImage(sketch), self.__loadImage(colored)\n",
    "    \n",
    "    def __loadImage(self, image_path):\n",
    "        return Image.open(os.path.join(self.data_folder, image_path))\n",
    "    \n",
    "    def get_color_hints(self, idx):\n",
    "        \"\"\"Get color hints for a specific image index without loading the full data item.\"\"\"\n",
    "        row = self.images.iloc[idx]\n",
    "        colored = row['Frame Path']\n",
    "        return self._get_or_extract_color_hints(colored)\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Save cache when the dataset object is destroyed\"\"\"\n",
    "        self._save_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04457a84-a021-48b8-938a-499c61289c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cache file color_hints_cache/color_hints_100_colors.json is corrupted. Creating new cache.\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/training'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c999b1-0fc7-438d-9062-0c945538f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(LightningModule):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg_model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.features = list(self.vgg.features[:16])\n",
    "        self.features = nn.Sequential(*self.features).eval()\n",
    "        \n",
    "        for params in self.features.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.criterion(self.features(x),self.features(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea516469-4e0d-4317-823d-c34304139ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorHintEmbedding(nn.Module):\n",
    "    def __init__(self, n_colors=50, color_dim=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.color_embedding = nn.Linear(color_dim, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, colors, positions=None):  # positions parameter kept for backward compatibility\n",
    "        # colors: [batch_size, n_colors, 3]\n",
    "        b, n, _ = colors.shape\n",
    "        color_embed = self.color_embedding(colors)  # [b, n, embed_dim]\n",
    "        return self.norm(color_embed)\n",
    "\n",
    "class ReferenceImageEncoder(nn.Module):\n",
    "    def __init__(self, transformer_dim=256):\n",
    "        super().__init__()\n",
    "        # Reduce initial channels and add more aggressive pooling\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),  # Stride=2 reduces spatial dim\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # Further reduction\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, transformer_dim, 3, stride=2, padding=1),  # Final reduction\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(8, transformer_dim)\n",
    "        \n",
    "        # Add adaptive pooling to ensure consistent output size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((16, 16))  # Fixed output size\n",
    "        \n",
    "    def forward(self, ref_image):\n",
    "        features = self.conv_blocks(ref_image)\n",
    "        features = self.norm(features)\n",
    "        features = self.adaptive_pool(features)  # Ensure consistent spatial dims\n",
    "        return features\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        \n",
    "        self.dim_head = 32\n",
    "        self.hidden_dim = self.dim_head * heads\n",
    "        \n",
    "        self.to_q = nn.Conv2d(dim, self.hidden_dim, 1, bias=False)\n",
    "        self.to_k = nn.Conv2d(dim, self.hidden_dim, 1, bias=False)\n",
    "        self.to_v = nn.Conv2d(dim, self.hidden_dim, 1, bias=False)\n",
    "        \n",
    "        # Add linear projections for non-spatial inputs\n",
    "        self.to_k_linear = nn.Linear(dim, self.hidden_dim)\n",
    "        self.to_v_linear = nn.Linear(dim, self.hidden_dim)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim, dim, 1),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        if context is not None:\n",
    "            # Handle non-spatial context (color hints)\n",
    "            if len(context.shape) == 3:  # [batch, n_colors, dim]\n",
    "                q = self.to_q(x)  # [b, hidden_dim, h, w]\n",
    "                k = self.to_k_linear(context)  # [b, n_colors, hidden_dim]\n",
    "                v = self.to_v_linear(context)  # [b, n_colors, hidden_dim]\n",
    "                \n",
    "                # Reshape query to [b, heads, h*w, dim_head]\n",
    "                q = q.reshape(b, self.heads, self.dim_head, h * w).transpose(-2, -1)\n",
    "                \n",
    "                # Reshape key and value to [b, heads, n_colors, dim_head]\n",
    "                k = k.reshape(b, -1, self.heads, self.dim_head).transpose(1, 2)\n",
    "                v = v.reshape(b, -1, self.heads, self.dim_head).transpose(1, 2)\n",
    "                \n",
    "            else:\n",
    "                # Handle spatial context (reference image)\n",
    "                if context.shape[-2:] != (h, w):\n",
    "                    context = F.interpolate(context, size=(h, w), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                q = self.to_q(x)\n",
    "                k = self.to_k(context)\n",
    "                v = self.to_v(context)\n",
    "                \n",
    "                # Reshape to [b, heads, h*w, dim_head]\n",
    "                q = q.reshape(b, self.heads, self.dim_head, -1).transpose(-2, -1)\n",
    "                k = k.reshape(b, self.heads, self.dim_head, -1).transpose(-2, -1)\n",
    "                v = v.reshape(b, self.heads, self.dim_head, -1).transpose(-2, -1)\n",
    "        else:\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(x)\n",
    "            v = self.to_v(x)\n",
    "            \n",
    "            # Reshape to [b, heads, h*w, dim_head]\n",
    "            q = q.reshape(b, self.heads, self.dim_head, -1).transpose(-2, -1)\n",
    "            k = k.reshape(b, self.heads, self.dim_head, -1).transpose(-2, -1)\n",
    "            v = v.reshape(b, self.heads, self.dim_head, -1).transpose(-2, -1)\n",
    "        \n",
    "        # Attention\n",
    "        dots = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        dots = dots - dots.max(dim=-1, keepdim=True)[0]\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        \n",
    "        # Apply attention and reshape\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(-2, -1).reshape(b, -1, h, w)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(dim, heads)\n",
    "        self.color_cross_attn = SelfAttention(dim, heads)\n",
    "        self.ref_cross_attn = SelfAttention(dim, heads)\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(8, dim)\n",
    "        self.norm2 = nn.GroupNorm(8, dim)\n",
    "        self.norm_color = nn.GroupNorm(8, dim)\n",
    "        self.norm_ref = nn.GroupNorm(8, dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim * 4, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(dim * 4, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, color_hints=None, ref_features=None):\n",
    "        # Self-attention\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        \n",
    "        # Cross-attention with color hints if provided\n",
    "        if color_hints is not None:\n",
    "            x = x + self.color_cross_attn(self.norm_color(x), color_hints)\n",
    "            \n",
    "        # Cross-attention with reference image features if provided\n",
    "        if ref_features is not None:\n",
    "            x = x + self.ref_cross_attn(self.norm_ref(x), ref_features)\n",
    "        \n",
    "        # FFN\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a371c0b5-5b32-4103-a6a2-c325760c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer(LightningModule):\n",
    "    def __init__(self, checkpoint_path=None, transformer_dim=256, transformer_heads=8):\n",
    "        super(Colorizer, self).__init__()\n",
    "        \n",
    "        if checkpoint_path is not None:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            \n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            \n",
    "            self.encoder.load_state_dict(\n",
    "                {k.replace('encoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('encoder.')}\n",
    "            )\n",
    "            self.decoder.load_state_dict(\n",
    "                {k.replace('decoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('decoder.')}\n",
    "            )\n",
    "            self.quant_conv.load_state_dict(\n",
    "                {k.replace('quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('quant_conv.')}\n",
    "            )\n",
    "            self.post_quant_conv.load_state_dict(\n",
    "                {k.replace('post_quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('post_quant_conv.')}\n",
    "            )\n",
    "            \n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            \n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.post_quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "            print(\"Loaded pretrained weights from checkpoint\")\n",
    "        else:\n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            print(\"Initialized new model from scratch\")\n",
    "        \n",
    "        # Initialize transformer and hint processing components\n",
    "        self.to_transformer_dim = nn.Conv2d(4, transformer_dim, 1)\n",
    "        self.transformer = TransformerBlock(transformer_dim, transformer_heads)\n",
    "        self.from_transformer_dim = nn.Conv2d(transformer_dim, 4, 1)\n",
    "        \n",
    "        # Add color hint and reference image processing\n",
    "        self.color_hint_processor = ColorHintEmbedding(\n",
    "            n_colors=50,\n",
    "            color_dim=3,\n",
    "            embed_dim=transformer_dim\n",
    "        )\n",
    "        self.ref_image_encoder = ReferenceImageEncoder(transformer_dim)\n",
    "        \n",
    "        # Training monitoring\n",
    "        self.num_high_loss_images = 50\n",
    "        self.high_loss_heap = []\n",
    "        self.current_min_high_loss = 0\n",
    "        \n",
    "        self.hparams.learning_rate = 0.0001\n",
    "\n",
    "    \n",
    "    def _freeze_autoencoder(self):\n",
    "        \"\"\"Freeze the autoencoder components.\"\"\"\n",
    "        components_to_freeze = [\n",
    "            self.encoder,\n",
    "            self.decoder,\n",
    "            self.quant_conv,\n",
    "            self.post_quant_conv\n",
    "        ]\n",
    "        \n",
    "        for component in components_to_freeze:\n",
    "            for param in component.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(\"Autoencoder components frozen\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint_path,\n",
    "        map_location=None,\n",
    "        strict=False,  # Changed to False by default\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom load_from_checkpoint to handle freezing after loading and skip position embeddings.\n",
    "        \"\"\"\n",
    "        # Load the checkpoint using parent class method\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
    "        \n",
    "        # Remove position embedding from state dict\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        position_keys = [k for k in state_dict.keys() if 'position_embedding' in k]\n",
    "        for k in position_keys:\n",
    "            del state_dict[k]\n",
    "        \n",
    "        # Create model instance\n",
    "        model = super(Colorizer, cls).load_from_checkpoint(\n",
    "            checkpoint_path,\n",
    "            map_location=map_location,\n",
    "            strict=False,  # Use non-strict loading\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Check if freeze_autoencoder is in hparams and apply freezing\n",
    "        if hasattr(model.hparams, 'freeze_autoencoder') and model.hparams.freeze_autoencoder:\n",
    "            model._freeze_autoencoder()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Only include trainable parameters\n",
    "        trainable_params = [p for p in self.parameters() if p.requires_grad]\n",
    "        return torch.optim.Adam(trainable_params, lr=self.hparams.learning_rate)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + std * eps\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, color_hints=None, ref_image=None):\n",
    "        z = self.post_quant_conv(z)\n",
    "        z = self.to_transformer_dim(z)\n",
    "        \n",
    "        # Process hints if provided\n",
    "        color_features = None\n",
    "        ref_features = None\n",
    "        \n",
    "        if color_hints is not None:\n",
    "            color_features = self.color_hint_processor(color_hints)\n",
    "            \n",
    "        if ref_image is not None:\n",
    "            ref_features = self.ref_image_encoder(ref_image)\n",
    "\n",
    "        z = self.transformer(z, color_features, ref_features)\n",
    "        \n",
    "        z = self.from_transformer_dim(z)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x, color_hints=None, ref_image=None):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z, color_hints, ref_image)\n",
    "        return x_recon\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Simplified batch handling\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            color_hints = None\n",
    "            ref_image = None\n",
    "        elif len(batch) == 3:\n",
    "            inputs, targets, ref_image = batch\n",
    "            color_hints = None\n",
    "        else:\n",
    "            inputs, targets, color_hints, ref_image = batch\n",
    "        \n",
    "        outputs = self(inputs, color_hints, ref_image)\n",
    "        \n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "        mse_loss = self.mse_loss_fn(outputs, targets)\n",
    "        total_loss = perceptual_loss + (mse_loss * 10)\n",
    "        \n",
    "        # Store high loss images with reference image and color hints\n",
    "        self.store_high_loss_image(total_loss, inputs, targets, outputs, ref_image, color_hints)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', total_loss)\n",
    "        self.log('perceptual_loss', perceptual_loss)\n",
    "        self.log('mse_loss', mse_loss)\n",
    "        \n",
    "        # Visualization logic\n",
    "        if batch_idx % 500 == 0:\n",
    "            # Visualize samples\n",
    "            num_images = min(4, inputs.shape[0])\n",
    "            for i in range(num_images):\n",
    "                # Create the main grid with input, output, target, and reference\n",
    "                grid = self.visualize_single_output(\n",
    "                    inputs[i],\n",
    "                    outputs[i],\n",
    "                    targets[i],\n",
    "                    ref_image[i] if ref_image is not None else None\n",
    "                )\n",
    "                \n",
    "                # Add color palette if color hints are present\n",
    "                if color_hints is not None:\n",
    "                    color_grid = self.visualize_color_hints(color_hints[i:i+1])\n",
    "                    if color_grid is not None:\n",
    "                        # Ensure color grid has same width as main grid\n",
    "                        target_width = grid.shape[2]\n",
    "                        color_height = int(color_grid.shape[1] * (target_width / color_grid.shape[2]))\n",
    "                        color_grid = F.interpolate(\n",
    "                            color_grid.unsqueeze(0), \n",
    "                            size=(color_height, target_width), \n",
    "                            mode='nearest'\n",
    "                        ).squeeze(0)\n",
    "                        \n",
    "                        # Concatenate vertically\n",
    "                        grid = torch.cat([grid, color_grid], dim=1)\n",
    "                \n",
    "                self.logger.experiment.add_image(\n",
    "                    f'Sample_Images/sample_{i+1}',\n",
    "                    grid,\n",
    "                    self.global_step\n",
    "                )\n",
    "            \n",
    "            # Visualize high loss images\n",
    "            self.visualize_high_loss_images(self.logger, self.global_step)\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def store_high_loss_image(self, loss, inputs, targets, outputs, ref_image=None, color_hints=None):\n",
    "        \"\"\"Store high loss images with reference image and color hints\"\"\"\n",
    "        # Convert to CPU and detach from computation graph\n",
    "        cpu_data = {\n",
    "            'loss': loss.item(),\n",
    "            'inputs': inputs.detach().cpu(),\n",
    "            'targets': targets.detach().cpu(),\n",
    "            'outputs': outputs.detach().cpu(),\n",
    "            'ref_image': ref_image.detach().cpu() if ref_image is not None else None,\n",
    "            'color_hints': color_hints.detach().cpu() if color_hints is not None else None\n",
    "        }\n",
    "        \n",
    "        if len(self.high_loss_heap) < self.num_high_loss_images:\n",
    "            heapq.heappush(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = min(loss.item(), self.current_min_high_loss if self.high_loss_heap else float('inf'))\n",
    "        elif loss.item() > self.current_min_high_loss:\n",
    "            heapq.heapreplace(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = self.high_loss_heap[0][0]\n",
    "\n",
    "    def visualize_high_loss_images(self, logger, step):\n",
    "        \"\"\"Visualize stored high loss images with reference images and color hints\"\"\"\n",
    "        if not self.high_loss_heap:\n",
    "            return\n",
    "            \n",
    "        # Sort by loss in descending order\n",
    "        sorted_entries = sorted(self.high_loss_heap, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Log each high-loss image separately\n",
    "        for idx, (loss_value, data) in enumerate(sorted_entries):\n",
    "            # Create main grid with images\n",
    "            grid = self.visualize_single_output(\n",
    "                data['inputs'],\n",
    "                data['outputs'],\n",
    "                data['targets'],\n",
    "                data['ref_image']\n",
    "            )\n",
    "            \n",
    "            # Add color palette if available\n",
    "            if data['color_hints'] is not None:\n",
    "                color_grid = self.visualize_color_hints(data['color_hints'])\n",
    "                if color_grid is not None:\n",
    "                    # Ensure color grid has same width as main grid\n",
    "                    target_width = grid.shape[2]\n",
    "                    color_height = int(color_grid.shape[1] * (target_width / color_grid.shape[2]))\n",
    "                    color_grid = F.interpolate(\n",
    "                        color_grid.unsqueeze(0), \n",
    "                        size=(color_height, target_width), \n",
    "                        mode='nearest'\n",
    "                    ).squeeze(0)\n",
    "                    \n",
    "                    # Concatenate vertically\n",
    "                    grid = torch.cat([grid, color_grid], dim=1)\n",
    "            \n",
    "            logger.experiment.add_image(\n",
    "                f'High_Loss_Images/image_{idx}',\n",
    "                grid,\n",
    "                step\n",
    "            )\n",
    "\n",
    "\n",
    "    def visualize_single_output(self, input_img, output_img, target_img, ref_image=None):\n",
    "        \"\"\"Helper function to create a grid with reference image if available\"\"\"\n",
    "        # Ensure we're working with batched images\n",
    "        if input_img.dim() == 3:\n",
    "            input_img = input_img.unsqueeze(0)\n",
    "            output_img = output_img.unsqueeze(0)\n",
    "            target_img = target_img.unsqueeze(0)\n",
    "            if ref_image is not None:\n",
    "                ref_image = ref_image.unsqueeze(0)\n",
    "        \n",
    "        # Create row with input, output, target, and reference image if available\n",
    "        images = [input_img, output_img, target_img]\n",
    "        if ref_image is not None:\n",
    "            images.append(ref_image)\n",
    "            \n",
    "        # Concatenate all images\n",
    "        row = torch.cat(images, dim=0)\n",
    "        \n",
    "        # Handle grayscale images\n",
    "        if row.shape[1] == 1:\n",
    "            row = row.repeat(1, 3, 1, 1)\n",
    "            \n",
    "        # Create grid with all images side by side\n",
    "        nrow = 4 if ref_image is not None else 3\n",
    "        grid = torchvision.utils.make_grid(row, nrow=nrow, normalize=True, padding=2)\n",
    "        return grid\n",
    "        \n",
    "    def visualize_color_hints(self, color_hints, size=64):\n",
    "        \"\"\"\n",
    "        Visualize color hints as a palette grid\n",
    "        Args:\n",
    "            color_hints: tensor of shape [batch_size, n_colors, 3] with values in range [0,1]\n",
    "            size: size of each color square in pixels\n",
    "        Returns:\n",
    "            grid: tensor of shape [3, H, W] representing the color palette grid\n",
    "        \"\"\"\n",
    "        if color_hints is None:\n",
    "            return None\n",
    "            \n",
    "        batch_size, n_colors, _ = color_hints.shape\n",
    "        \n",
    "        # Create square patches for each color\n",
    "        patches = []\n",
    "        for b in range(batch_size):\n",
    "            # Create a row of color patches for this batch\n",
    "            batch_patches = []\n",
    "            for c in range(n_colors):\n",
    "                # Create a square patch of the current color\n",
    "                color = color_hints[b, c]\n",
    "                patch = color.view(3, 1, 1).repeat(1, size, size)\n",
    "                batch_patches.append(patch)\n",
    "            \n",
    "            # Stack patches horizontally for this batch\n",
    "            row = torch.cat(batch_patches, dim=2)\n",
    "            patches.append(row)\n",
    "        \n",
    "        # Stack all batches vertically\n",
    "        grid = torch.cat(patches, dim=1)\n",
    "        \n",
    "        return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b02076-c894-4cf9-bc6e-9199a07cee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpt_file = 'checkpoints/version_16.ckpt'\n",
    "# model = Colorizer(chkpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12d7b78-f637-4b16-bd93-7c707fdc6374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized new model from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['transformer.attention.to_k_linear.weight', 'transformer.attention.to_k_linear.bias', 'transformer.attention.to_v_linear.weight', 'transformer.attention.to_v_linear.bias', 'transformer.color_cross_attn.to_k_linear.weight', 'transformer.color_cross_attn.to_k_linear.bias', 'transformer.color_cross_attn.to_v_linear.weight', 'transformer.color_cross_attn.to_v_linear.bias', 'transformer.ref_cross_attn.to_k_linear.weight', 'transformer.ref_cross_attn.to_k_linear.bias', 'transformer.ref_cross_attn.to_v_linear.weight', 'transformer.ref_cross_attn.to_v_linear.bias']\n",
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['color_hint_processor.position_embedding.weight']\n"
     ]
    }
   ],
   "source": [
    "chkpt_file = 'checkpoints/image-hint-frozen-vae-transformer-v1.ckpt'\n",
    "model = Colorizer.load_from_checkpoint(chkpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f63dc6bb-a4d1-403a-93b7-78cb980697d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cache file color_hints_cache/color_hints_100_colors.json is corrupted. Creating new cache.\n"
     ]
    }
   ],
   "source": [
    "# data_folder = 'data/toy'\n",
    "data_folder = 'data/training'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f9efaf-17f4-4895-819f-d2ae21cd618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = loggers.TensorBoardLogger(\"tb_logs\", name='color-hint-frozen-vae-transformer')\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=20, logger=logger, log_every_n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce4e5-a70b-4e74-ab36-3fa834cbb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                 | Type                  | Params\n",
      "----------------------------------------------------------------\n",
      "0  | encoder              | Encoder               | 34.2 M\n",
      "1  | decoder              | Decoder               | 49.5 M\n",
      "2  | quant_conv           | Conv2d                | 72    \n",
      "3  | post_quant_conv      | Conv2d                | 20    \n",
      "4  | loss_fn              | VGGPerceptualLoss     | 138 M \n",
      "5  | mse_loss_fn          | MSELoss               | 0     \n",
      "6  | to_transformer_dim   | Conv2d                | 1.3 K \n",
      "7  | transformer          | TransformerBlock      | 1.7 M \n",
      "8  | from_transformer_dim | Conv2d                | 1.0 K \n",
      "9  | color_hint_processor | ColorHintEmbedding    | 1.5 K \n",
      "10 | ref_image_encoder    | ReferenceImageEncoder | 167 K \n",
      "----------------------------------------------------------------\n",
      "222 M     Trainable params\n",
      "1.7 M     Non-trainable params\n",
      "223 M     Total params\n",
      "895.570   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|██▎                                                                                                                                                | 1994/129629 [21:17<22:43:07,  1.56it/s, v_num=8]Saving cache after 1000 updates...\n",
      "Epoch 0:   2%|██▎                                                                                                                                                | 1995/129629 [21:18<22:43:03,  1.56it/s, v_num=8]Saving cache after 1000 updates...\n",
      "Epoch 0:   2%|██▎                                                                                                                                                | 1996/129629 [21:18<22:42:58,  1.56it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:   3%|████▌                                                                                                                                              | 3994/129629 [43:15<22:40:44,  1.54it/s, v_num=8]Saving cache after 2000 updates...\n",
      "Epoch 0:   3%|████▌                                                                                                                                              | 3995/129629 [43:16<22:40:41,  1.54it/s, v_num=8]Saving cache after 2000 updates...\n",
      "Epoch 0:   3%|████▌                                                                                                                                              | 3996/129629 [43:16<22:40:38,  1.54it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:   3%|▎         | 4133/129629 [45:14<22:53:53,  1.52it/s, v_num=8]                                                                                                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|██        | 25994/129629 [4:46:09<19:00:53,  1.51it/s, v_num=8]Saving cache after 13000 updates...\n",
      "Epoch 0:  20%|██        | 25995/129629 [4:46:10<19:00:52,  1.51it/s, v_num=8]Saving cache after 13000 updates...\n",
      "Epoch 0:  20%|██        | 25998/129629 [4:46:16<19:01:07,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  22%|██▏       | 27994/129629 [5:08:15<18:39:11,  1.51it/s, v_num=8]Saving cache after 14000 updates...\n",
      "Epoch 0:  22%|██▏       | 27995/129629 [5:08:16<18:39:10,  1.51it/s, v_num=8]Saving cache after 14000 updates...\n",
      "Epoch 0:  23%|██▎       | 29581/129629 [5:26:17<18:23:32,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  29%|██▉       | 37994/129629 [6:58:39<16:49:44,  1.51it/s, v_num=8]Saving cache after 19000 updates...\n",
      "Epoch 0:  29%|██▉       | 37995/129629 [6:58:40<16:49:43,  1.51it/s, v_num=8]Saving cache after 19000 updates...\n",
      "Epoch 0:  29%|██▉       | 37998/129629 [6:58:42<16:49:42,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  30%|██▉       | 38605/129629 [7:06:01<16:44:28,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  35%|███▌      | 45994/129629 [8:27:11<15:22:16,  1.51it/s, v_num=8]Saving cache after 23000 updates...\n",
      "Epoch 0:  35%|███▌      | 45995/129629 [8:27:12<15:22:15,  1.51it/s, v_num=8]Saving cache after 23000 updates...\n",
      "Epoch 0:  35%|███▌      | 45998/129629 [8:27:14<15:22:15,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  36%|███▌      | 46304/129629 [8:31:02<15:19:37,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  39%|███▊      | 49994/129629 [9:11:57<14:39:11,  1.51it/s, v_num=8]Saving cache after 25000 updates...\n",
      "Epoch 0:  39%|███▊      | 49995/129629 [9:11:57<14:39:11,  1.51it/s, v_num=8]Saving cache after 25000 updates...\n",
      "Epoch 0:  39%|███▊      | 49998/129629 [9:12:00<14:39:10,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  40%|███▉      | 51457/129629 [9:28:21<14:23:26,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  51%|█████     | 65994/129629 [12:10:31<11:44:24,  1.51it/s, v_num=8]Saving cache after 33000 updates...\n",
      "Epoch 0:  51%|█████     | 65995/129629 [12:10:31<11:44:23,  1.51it/s, v_num=8]Saving cache after 33000 updates...\n",
      "Epoch 0:  51%|█████     | 65998/129629 [12:10:34<11:44:22,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  51%|█████▏    | 66584/129629 [12:17:49<11:38:36,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  69%|██████▉   | 89851/129629 [16:38:03<7:21:51,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|████████▋ | 111994/129629 [20:47:34<3:16:26,  1.50it/s, v_num=8]Saving cache after 56000 updates...\n",
      "Epoch 0:  86%|████████▋ | 111995/129629 [20:47:35<3:16:26,  1.50it/s, v_num=8]Saving cache after 56000 updates...\n",
      "Epoch 0:  86%|████████▋ | 111998/129629 [20:47:38<3:16:24,  1.50it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  87%|████████▋ | 113255/129629 [21:02:20<3:02:30,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  91%|█████████ | 117994/129629 [21:55:26<2:09:42,  1.49it/s, v_num=8]Saving cache after 59000 updates...\n",
      "Epoch 0:  91%|█████████ | 117995/129629 [21:55:27<2:09:42,  1.49it/s, v_num=8]Saving cache after 59000 updates...\n",
      "Epoch 0:  91%|█████████ | 117998/129629 [21:55:30<2:09:40,  1.49it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  92%|█████████▏| 118900/129629 [22:06:09<1:59:40,  1.49it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  93%|█████████▎| 119994/129629 [22:18:06<1:47:26,  1.49it/s, v_num=8]Saving cache after 60000 updates...\n",
      "Epoch 0:  93%|█████████▎| 119995/129629 [22:18:07<1:47:26,  1.49it/s, v_num=8]Saving cache after 60000 updates...\n",
      "Epoch 0:  93%|█████████▎| 119998/129629 [22:18:10<1:47:24,  1.49it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  94%|█████████▍| 121994/129629 [22:40:45<1:25:09,  1.49it/s, v_num=8]Saving cache after 61000 updates...\n",
      "Epoch 0:  94%|█████████▍| 121995/129629 [22:40:45<1:25:09,  1.49it/s, v_num=8]Saving cache after 61000 updates...\n",
      "Epoch 0:  94%|█████████▍| 121998/129629 [22:40:48<1:25:07,  1.49it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  96%|█████████▌| 123994/129629 [23:03:27<1:02:52,  1.49it/s, v_num=8]Saving cache after 62000 updates...\n",
      "Epoch 0:  96%|█████████▌| 123995/129629 [23:03:27<1:02:51,  1.49it/s, v_num=8]Saving cache after 62000 updates...\n",
      "Epoch 0:  96%|█████████▌| 123998/129629 [23:03:30<1:02:49,  1.49it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  97%|█████████▋| 125690/129629 [23:23:16<43:58,  1.49it/s, v_num=8]  Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  99%|█████████▊| 127994/129629 [23:49:03<18:15,  1.49it/s, v_num=8]Saving cache after 64000 updates...\n",
      "Epoch 0:  99%|█████████▊| 127995/129629 [23:49:04<18:14,  1.49it/s, v_num=8]Saving cache after 64000 updates...\n",
      "Epoch 0:  99%|█████████▊| 127998/129629 [23:49:06<18:12,  1.49it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 0:  99%|█████████▉| 128288/129629 [23:53:08<14:58,  1.49it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 21509/129629 [3:58:02<19:56:34,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▊        | 23994/129629 [4:24:47<19:25:45,  1.51it/s, v_num=8]Saving cache after 12000 updates...\n",
      "Epoch 1:  19%|█▊        | 23995/129629 [4:24:47<19:25:44,  1.51it/s, v_num=8]Saving cache after 12000 updates...\n",
      "Epoch 1:  19%|█▊        | 23999/129629 [4:24:55<19:26:02,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 1:  20%|██        | 25994/129629 [4:47:01<19:04:21,  1.51it/s, v_num=8]Saving cache after 13000 updates...\n",
      "Epoch 1:  20%|██        | 25995/129629 [4:47:02<19:04:20,  1.51it/s, v_num=8]Saving cache after 13000 updates...\n",
      "Epoch 1:  20%|██        | 25999/129629 [4:47:10<19:04:37,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 1:  22%|██▏       | 27994/129629 [5:09:09<18:42:27,  1.51it/s, v_num=8]Saving cache after 14000 updates...\n",
      "Epoch 1:  22%|██▏       | 27995/129629 [5:09:10<18:42:26,  1.51it/s, v_num=8]Saving cache after 14000 updates...\n",
      "Epoch 1:  22%|██▏       | 27999/129629 [5:09:18<18:42:43,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 1:  22%|██▏       | 28899/129629 [5:19:23<18:33:15,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  39%|███▊      | 49994/129629 [9:12:37<14:40:16,  1.51it/s, v_num=8]Saving cache after 25000 updates...\n",
      "Epoch 1:  39%|███▊      | 49995/129629 [9:12:38<14:40:15,  1.51it/s, v_num=8]Saving cache after 25000 updates...\n",
      "Epoch 1:  39%|███▊      | 49999/129629 [9:12:52<14:40:31,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 1:  39%|███▉      | 51006/129629 [9:24:34<14:30:15,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|████      | 51994/129629 [9:34:45<14:18:11,  1.51it/s, v_num=8]Saving cache after 26000 updates...\n",
      "Epoch 1:  40%|████      | 51995/129629 [9:34:45<14:18:10,  1.51it/s, v_num=8]Saving cache after 26000 updates...\n",
      "Epoch 1:  40%|████      | 51998/129629 [9:34:48<14:18:10,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 1:  40%|████      | 52283/129629 [9:38:25<14:15:42,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  57%|█████▋    | 73994/129629 [13:39:04<10:15:51,  1.51it/s, v_num=8]Saving cache after 37000 updates...\n",
      "Epoch 1:  57%|█████▋    | 73995/129629 [13:39:05<10:15:50,  1.51it/s, v_num=8]Saving cache after 37000 updates...\n",
      "Epoch 1:  57%|█████▋    | 73999/129629 [13:39:26<10:16:01,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 1:  57%|█████▋    | 74240/129629 [13:42:25<10:13:36,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 75575/129629 [13:57:20<9:58:53,  1.50it/s, v_num=8] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  76%|███████▌  | 97956/129629 [18:07:18<5:51:34,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 99193/129629 [18:21:39<5:38:01,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  94%|█████████▍| 121640/129629 [22:34:57<1:28:59,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|█████████▍| 122828/129629 [22:48:23<1:15:46,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  12%|█▏        | 15994/129629 [2:56:58<20:57:23,  1.51it/s, v_num=8]Saving cache after 8000 updates...\n",
      "Epoch 2:  12%|█▏        | 15995/129629 [2:56:59<20:57:22,  1.51it/s, v_num=8]Saving cache after 8000 updates...\n",
      "Epoch 2:  12%|█▏        | 15997/129629 [2:57:00<20:57:20,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 2:  13%|█▎        | 16342/129629 [3:01:02<20:55:02,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  19%|█▊        | 23994/129629 [4:25:18<19:28:00,  1.51it/s, v_num=8]Saving cache after 12000 updates...\n",
      "Epoch 2:  19%|█▊        | 23995/129629 [4:25:18<19:27:59,  1.51it/s, v_num=8]Saving cache after 12000 updates...\n",
      "Epoch 2:  19%|█▊        | 23998/129629 [4:25:21<19:28:00,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 2:  20%|██        | 25994/129629 [4:47:18<19:05:27,  1.51it/s, v_num=8]Saving cache after 13000 updates...\n",
      "Epoch 2:  20%|██        | 25995/129629 [4:47:18<19:05:26,  1.51it/s, v_num=8]Saving cache after 13000 updates...\n",
      "Epoch 2:  20%|██        | 25998/129629 [4:47:21<19:05:25,  1.51it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 2:  20%|██        | 26532/129629 [4:53:51<19:01:52,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  38%|███▊      | 49722/129629 [9:09:25<14:42:58,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  57%|█████▋    | 73768/129629 [13:35:42<10:17:41,  1.51it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  74%|███████▎  | 95373/129629 [17:36:38<6:19:31,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  76%|███████▌  | 97913/129629 [18:04:46<5:51:22,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  92%|█████████▏| 119198/129629 [22:03:46<1:55:50,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  94%|█████████▍| 121791/129629 [22:32:38<1:27:03,  1.50it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  10%|█         | 13552/129629 [2:28:40<21:13:22,  1.52it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  12%|█▏        | 15994/129629 [2:54:51<20:42:23,  1.52it/s, v_num=8]Saving cache after 8000 updates...\n",
      "Epoch 3:  12%|█▏        | 15995/129629 [2:54:52<20:42:22,  1.52it/s, v_num=8]Saving cache after 8000 updates...\n",
      "Epoch 3:  12%|█▏        | 15999/129629 [2:54:57<20:42:34,  1.52it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 3:  12%|█▏        | 16201/129629 [2:57:31<20:42:53,  1.52it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  19%|█▊        | 23994/129629 [4:22:51<19:17:16,  1.52it/s, v_num=8]Saving cache after 12000 updates...\n",
      "Epoch 3:  19%|█▊        | 23995/129629 [4:22:52<19:17:15,  1.52it/s, v_num=8]Saving cache after 12000 updates...\n",
      "Epoch 3:  19%|█▊        | 23999/129629 [4:22:59<19:17:31,  1.52it/s, v_num=8]Error saving cache: [Errno 2] No such file or directory: 'color_hints_cache/color_hints_100_colors.json.tmp' -> 'color_hints_cache/color_hints_100_colors.json'\n",
      "Epoch 3:  19%|█▊        | 24120/129629 [4:24:46<19:18:11,  1.52it/s, v_num=8]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be137035-d800-49d2-aa32-a1b56fcc6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def viewTensor(output):\n",
    "    image = to_pil_image(output.squeeze())\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af58b2-33bb-4b65-8644-48112b2b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data_folder = 'data/test'\n",
    "data_csv = 'data.csv'\n",
    "test_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabfd6-8430-4466-a3e4-a707b7cac420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "x, y = test_dataset[idx]\n",
    "output = model(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd70dc-50de-4bcf-8f95-8a4c0503ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1268b-3ee2-4167-a450-0068e6bfa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18232e0-e302-44e3-8ca9-6d59d02e91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a514-33e9-42c7-989d-96397ed4d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0402584-06d0-4746-aa54-bc87394577eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
