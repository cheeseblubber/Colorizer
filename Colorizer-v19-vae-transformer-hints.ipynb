{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62340f-265b-4a2a-8032-c4d78cbbe551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from anything_vae import (\n",
    "    Encoder,\n",
    "    Decoder,\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models as torchvision_models\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers, callbacks\n",
    "# import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import heapq\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501fa544-ca50-4173-8ac0-00d5f5023f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, data_folder, data_csv, transform=None, hint_offset=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_folder (string): Directory with all the images.\n",
    "            data_csv (string): CSV file with image paths.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            hint_offset (int): Number of images away to fetch the hint image.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.data_path = os.path.join(data_folder, data_csv)\n",
    "        self.images = pd.read_csv(self.data_path)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB by replicating channels\n",
    "            transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "        ])\n",
    "\n",
    "        self.tranform_output = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        self.hint_offset = hint_offset\n",
    "\n",
    "        # Extract show names from the file paths\n",
    "        self.images['show'] = self.images['Sketch Path'].apply(\n",
    "            lambda x: os.path.basename(os.path.dirname(x))\n",
    "        )\n",
    "\n",
    "        # Sort the DataFrame by show to group images from the same show\n",
    "        self.images = self.images.sort_values(by=['show']).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.images.iloc[idx]\n",
    "        sketch = row['Sketch Path']\n",
    "        colored = row['Frame Path']\n",
    "        show = row['show']\n",
    "\n",
    "        # Get indices of all images from the same show\n",
    "        show_indices = self.images.index[self.images['show'] == show].tolist()\n",
    "\n",
    "        # Find the position of the current image within the show's indices\n",
    "        pos_in_show = show_indices.index(idx)\n",
    "\n",
    "        # Calculate hint index with offset and ensure it's within bounds\n",
    "        hint_pos_in_show = pos_in_show + self.hint_offset\n",
    "        hint_pos_in_show = max(0, min(hint_pos_in_show, len(show_indices) - 1))\n",
    "\n",
    "        # Get the actual index of the hint image in self.images\n",
    "        hint_idx = show_indices[hint_pos_in_show]\n",
    "\n",
    "        hint_row = self.images.iloc[hint_idx]\n",
    "        hint_sketch = hint_row['Sketch Path']\n",
    "        hint_colored = hint_row['Frame Path']\n",
    "\n",
    "        # Load images\n",
    "        sketch_image = self.transform(self.__loadImage(sketch))\n",
    "        colored_image = self.tranform_output(self.__loadImage(colored))\n",
    "        hint_image = self.tranform_output(self.__loadImage(hint_colored))\n",
    "\n",
    "        return sketch_image, colored_image, hint_image\n",
    "\n",
    "    def viewImage(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx][['Sketch Path', 'Frame Path']]\n",
    "        return self.__loadImage(sketch), self.__loadImage(colored)\n",
    "\n",
    "    def __loadImage(self, image_path):\n",
    "        return Image.open(os.path.join(self.data_folder, image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c999b1-0fc7-438d-9062-0c945538f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(LightningModule):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg_model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.features = list(self.vgg.features[:16])\n",
    "        self.features = nn.Sequential(*self.features).eval()\n",
    "        \n",
    "        for params in self.features.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.criterion(self.features(x),self.features(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea516469-4e0d-4317-823d-c34304139ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorHintEmbedding(nn.Module):\n",
    "    def __init__(self, n_colors=50, color_dim=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.color_embedding = nn.Linear(color_dim, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(n_colors, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, colors, positions):\n",
    "        # colors: [batch_size, n_colors, 3]\n",
    "        # positions: [batch_size, n_colors, 2] (x,y coordinates)\n",
    "        b, n, _ = colors.shape\n",
    "        color_embed = self.color_embedding(colors)  # [b, n, embed_dim]\n",
    "        pos_embed = self.position_embedding(positions)  # [b, n, embed_dim]\n",
    "        return self.norm(color_embed + pos_embed)\n",
    "\n",
    "class ReferenceImageEncoder(nn.Module):\n",
    "    def __init__(self, transformer_dim=256):\n",
    "        super().__init__()\n",
    "        # Reduce initial channels and add more aggressive pooling\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),  # Stride=2 reduces spatial dim\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # Further reduction\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, transformer_dim, 3, stride=2, padding=1),  # Final reduction\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(8, transformer_dim)\n",
    "        \n",
    "        # Add adaptive pooling to ensure consistent output size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((16, 16))  # Fixed output size\n",
    "        \n",
    "    def forward(self, ref_image):\n",
    "        features = self.conv_blocks(ref_image)\n",
    "        features = self.norm(features)\n",
    "        features = self.adaptive_pool(features)  # Ensure consistent spatial dims\n",
    "        return features\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        \n",
    "        # Reduce internal dimension\n",
    "        self.dim_head = 32  # Reduced from dim // heads\n",
    "        self.hidden_dim = self.dim_head * heads\n",
    "        \n",
    "        # Use smaller internal dimensions for QKV projections\n",
    "        self.to_q = nn.Conv2d(dim, self.hidden_dim, 1, bias=False)\n",
    "        self.to_k = nn.Conv2d(dim, self.hidden_dim, 1, bias=False)\n",
    "        self.to_v = nn.Conv2d(dim, self.hidden_dim, 1, bias=False)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(self.hidden_dim, dim, 1),\n",
    "            nn.Dropout(0.1)  # Add dropout for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        if context is not None:\n",
    "            # Resize context if needed\n",
    "            if context.shape[-2:] != (h, w):\n",
    "                context = F.interpolate(context, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(context)\n",
    "            v = self.to_v(context)\n",
    "        else:\n",
    "            q = self.to_q(x)\n",
    "            k = self.to_k(x)\n",
    "            v = self.to_v(x)\n",
    "            \n",
    "        # Reshape with reduced dimension\n",
    "        q = q.reshape(b, self.heads, self.dim_head, -1)\n",
    "        k = k.reshape(b, self.heads, self.dim_head, -1)\n",
    "        v = v.reshape(b, self.heads, self.dim_head, -1)\n",
    "        \n",
    "        # Efficient attention computation\n",
    "        dots = torch.matmul(q.transpose(-2, -1), k) * self.scale\n",
    "        \n",
    "        # Use memory efficient softmax\n",
    "        dots = dots - dots.max(dim=-1, keepdim=True)[0]  # Numerical stability\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        \n",
    "        # Optional memory optimization: use chunked matrix multiplication\n",
    "        out = torch.matmul(attn, v.transpose(-2, -1))\n",
    "        out = out.reshape(b, -1, h, w)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(dim, heads)\n",
    "        self.color_cross_attn = SelfAttention(dim, heads)\n",
    "        self.ref_cross_attn = SelfAttention(dim, heads)\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(8, dim)\n",
    "        self.norm2 = nn.GroupNorm(8, dim)\n",
    "        self.norm_color = nn.GroupNorm(8, dim)\n",
    "        self.norm_ref = nn.GroupNorm(8, dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim * 4, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(dim * 4, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, color_hints=None, ref_features=None):\n",
    "        # Self-attention\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        \n",
    "        # Cross-attention with color hints if provided\n",
    "        if color_hints is not None:\n",
    "            b, c, h, w = x.shape\n",
    "            # Reshape color hints to spatial dimension\n",
    "            color_hints = color_hints.view(b, c, -1).permute(0, 2, 1)\n",
    "            color_hints = color_hints.view(b, c, h, w)\n",
    "            x = x + self.color_cross_attn(self.norm_color(x), color_hints)\n",
    "            \n",
    "        # Cross-attention with reference image features if provided\n",
    "        if ref_features is not None:\n",
    "            x = x + self.ref_cross_attn(self.norm_ref(x), ref_features)\n",
    "        \n",
    "        # FFN\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a371c0b5-5b32-4103-a6a2-c325760c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer(LightningModule):\n",
    "    def __init__(self, checkpoint_path=None, transformer_dim=256, transformer_heads=8):\n",
    "        super(Colorizer, self).__init__()\n",
    "        \n",
    "        if checkpoint_path is not None:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            \n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            \n",
    "            self.encoder.load_state_dict(\n",
    "                {k.replace('encoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('encoder.')}\n",
    "            )\n",
    "            self.decoder.load_state_dict(\n",
    "                {k.replace('decoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('decoder.')}\n",
    "            )\n",
    "            self.quant_conv.load_state_dict(\n",
    "                {k.replace('quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('quant_conv.')}\n",
    "            )\n",
    "            self.post_quant_conv.load_state_dict(\n",
    "                {k.replace('post_quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('post_quant_conv.')}\n",
    "            )\n",
    "            \n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            \n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.post_quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "            print(\"Loaded pretrained weights from checkpoint\")\n",
    "        else:\n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            print(\"Initialized new model from scratch\")\n",
    "        \n",
    "        # Initialize transformer and hint processing components\n",
    "        self.to_transformer_dim = nn.Conv2d(4, transformer_dim, 1)\n",
    "        self.transformer = TransformerBlock(transformer_dim, transformer_heads)\n",
    "        self.from_transformer_dim = nn.Conv2d(transformer_dim, 4, 1)\n",
    "        \n",
    "        # Add color hint and reference image processing\n",
    "        self.color_hint_processor = ColorHintEmbedding(\n",
    "            n_colors=50,\n",
    "            color_dim=3,\n",
    "            embed_dim=transformer_dim\n",
    "        )\n",
    "        self.ref_image_encoder = ReferenceImageEncoder(transformer_dim)\n",
    "        \n",
    "        # Training monitoring\n",
    "        self.num_high_loss_images = 50\n",
    "        self.high_loss_heap = []\n",
    "        self.current_min_high_loss = 0\n",
    "        \n",
    "        self.hparams.learning_rate = 0.0001\n",
    "\n",
    "    \n",
    "    def _freeze_autoencoder(self):\n",
    "        \"\"\"Freeze the autoencoder components.\"\"\"\n",
    "        components_to_freeze = [\n",
    "            self.encoder,\n",
    "            self.decoder,\n",
    "            self.quant_conv,\n",
    "            self.post_quant_conv\n",
    "        ]\n",
    "        \n",
    "        for component in components_to_freeze:\n",
    "            for param in component.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Only include trainable parameters\n",
    "        trainable_params = [p for p in self.parameters() if p.requires_grad]\n",
    "        return torch.optim.Adam(trainable_params, lr=self.hparams.learning_rate)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint_path,\n",
    "        map_location=None,\n",
    "        strict=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom load_from_checkpoint to handle freezing after loading.\n",
    "        \"\"\"\n",
    "        # Load the checkpoint using parent class method\n",
    "        model = super().load_from_checkpoint(\n",
    "            checkpoint_path,\n",
    "            map_location=map_location,\n",
    "            strict=strict,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Reapply freezing if specified\n",
    "        if model.hparams.freeze_autoencoder:\n",
    "            model._freeze_autoencoder()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + std * eps\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, color_hints=None, ref_image=None):\n",
    "        z = self.post_quant_conv(z)\n",
    "        z = self.to_transformer_dim(z)\n",
    "        \n",
    "        # Process hints if provided\n",
    "        color_features = None\n",
    "        ref_features = None\n",
    "        \n",
    "        if color_hints is not None:\n",
    "            colors, positions = color_hints\n",
    "            color_features = self.color_hint_processor(colors, positions)\n",
    "            \n",
    "        if ref_image is not None:\n",
    "            ref_features = self.ref_image_encoder(ref_image)\n",
    "\n",
    "        z = self.transformer(z, color_features, ref_features)\n",
    "        \n",
    "        z = self.from_transformer_dim(z)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x, color_hints=None, ref_image=None):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z, color_hints, ref_image)\n",
    "        return x_recon\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack batch - now supports hints and reference images\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            color_hints = None\n",
    "            ref_image = None\n",
    "        elif len(batch) == 3:\n",
    "            inputs, targets, ref_image = batch\n",
    "            color_hints = None\n",
    "        else:\n",
    "            inputs, targets, color_hints, ref_image = batch\n",
    "        \n",
    "        outputs = self(inputs, color_hints, ref_image)\n",
    "        \n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "        mse_loss = self.mse_loss_fn(outputs, targets)\n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        \n",
    "        # Store high loss images with reference image\n",
    "        self.store_high_loss_image(total_loss, inputs, targets, outputs, ref_image)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', total_loss)\n",
    "        self.log('perceptual_loss', perceptual_loss)\n",
    "        self.log('mse_loss', mse_loss)\n",
    "        \n",
    "        # Visualization logic\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            self.visualize_high_loss_images(self.logger, self.global_step)\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            num_images = min(4, inputs.shape[0])\n",
    "            for i in range(num_images):\n",
    "                grid = self.visualize_single_output(\n",
    "                    inputs[i],\n",
    "                    outputs[i],\n",
    "                    targets[i],\n",
    "                    ref_image[i] if ref_image is not None else None\n",
    "                )\n",
    "                self.logger.experiment.add_image(\n",
    "                    f'Sample_Images/sample_{i+1}',\n",
    "                    grid,\n",
    "                    self.global_step\n",
    "                )\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def visualize_high_loss_images(self, logger, step):\n",
    "        \"\"\"Visualize stored high loss images with reference images if available\"\"\"\n",
    "        if not self.high_loss_heap:\n",
    "            return\n",
    "            \n",
    "        # Sort by loss in descending order\n",
    "        sorted_entries = sorted(self.high_loss_heap, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Log each high-loss image separately\n",
    "        for idx, (loss_value, data) in enumerate(sorted_entries):\n",
    "            grid = self.visualize_single_output(\n",
    "                data['inputs'],\n",
    "                data['outputs'],\n",
    "                data['targets'],\n",
    "                data['ref_image']\n",
    "            )\n",
    "            \n",
    "            logger.experiment.add_image(\n",
    "                f'High_Loss_Images/image',\n",
    "                grid,\n",
    "                step\n",
    "            )\n",
    "\n",
    "\n",
    "    def store_high_loss_image(self, loss, inputs, targets, outputs, ref_image=None):\n",
    "        \"\"\"Store high loss images with reference image if available\"\"\"\n",
    "        # Convert to CPU and detach from computation graph\n",
    "        cpu_data = {\n",
    "            'loss': loss.item(),\n",
    "            'inputs': inputs.detach().cpu(),\n",
    "            'targets': targets.detach().cpu(),\n",
    "            'outputs': outputs.detach().cpu(),\n",
    "            'ref_image': ref_image.detach().cpu() if ref_image is not None else None\n",
    "        }\n",
    "        \n",
    "        if len(self.high_loss_heap) < self.num_high_loss_images:\n",
    "            heapq.heappush(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = min(loss.item(), self.current_min_high_loss if self.high_loss_heap else float('inf'))\n",
    "        elif loss.item() > self.current_min_high_loss:\n",
    "            heapq.heapreplace(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = self.high_loss_heap[0][0]\n",
    "\n",
    "\n",
    "    def visualize_single_output(self, input_img, output_img, target_img, ref_image=None):\n",
    "        \"\"\"Helper function to create a grid with reference image if available\"\"\"\n",
    "        # Ensure we're working with batched images\n",
    "        if input_img.dim() == 3:\n",
    "            input_img = input_img.unsqueeze(0)\n",
    "            output_img = output_img.unsqueeze(0)\n",
    "            target_img = target_img.unsqueeze(0)\n",
    "            if ref_image is not None:\n",
    "                ref_image = ref_image.unsqueeze(0)\n",
    "        \n",
    "        # Create row with input, output, target, and reference image if available\n",
    "        images = [input_img, output_img, target_img]\n",
    "        if ref_image is not None:\n",
    "            images.append(ref_image)\n",
    "            \n",
    "        # Concatenate all images\n",
    "        row = torch.cat(images, dim=0)\n",
    "        \n",
    "        # Handle grayscale images\n",
    "        if row.shape[1] == 1:\n",
    "            row = row.repeat(1, 3, 1, 1)\n",
    "            \n",
    "        # Create grid with all images side by side\n",
    "        nrow = 4 if ref_image is not None else 3\n",
    "        grid = torchvision.utils.make_grid(row, nrow=nrow, normalize=True, padding=2)\n",
    "        return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b02076-c894-4cf9-bc6e-9199a07cee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights from checkpoint\n"
     ]
    }
   ],
   "source": [
    "chkpt_file = 'checkpoints/version_16.ckpt'\n",
    "model = Colorizer(chkpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63dc6bb-a4d1-403a-93b7-78cb980697d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = 'data/toy'\n",
    "data_folder = 'data/training'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f9efaf-17f4-4895-819f-d2ae21cd618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = loggers.TensorBoardLogger(\"tb_logs\", name='image-hint-frozen-vae-transformer')\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=20, logger=logger, log_every_n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce4e5-a70b-4e74-ab36-3fa834cbb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                 | Type                  | Params\n",
      "----------------------------------------------------------------\n",
      "0  | encoder              | Encoder               | 34.2 M\n",
      "1  | decoder              | Decoder               | 49.5 M\n",
      "2  | quant_conv           | Conv2d                | 72    \n",
      "3  | post_quant_conv      | Conv2d                | 20    \n",
      "4  | loss_fn              | VGGPerceptualLoss     | 138 M \n",
      "5  | mse_loss_fn          | MSELoss               | 0     \n",
      "6  | to_transformer_dim   | Conv2d                | 1.3 K \n",
      "7  | transformer          | TransformerBlock      | 1.3 M \n",
      "8  | from_transformer_dim | Conv2d                | 1.0 K \n",
      "9  | color_hint_processor | ColorHintEmbedding    | 14.3 K\n",
      "10 | ref_image_encoder    | ReferenceImageEncoder | 167 K \n",
      "----------------------------------------------------------------\n",
      "138 M     Trainable params\n",
      "85.4 M    Non-trainable params\n",
      "223 M     Total params\n",
      "894.042   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  64%|███████████████████████▌             | 82499/129629 [16:06:09<9:11:57,  1.42it/s, v_num=1]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be137035-d800-49d2-aa32-a1b56fcc6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def viewTensor(output):\n",
    "    image = to_pil_image(output.squeeze())\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af58b2-33bb-4b65-8644-48112b2b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data_folder = 'data/test'\n",
    "data_csv = 'data.csv'\n",
    "test_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabfd6-8430-4466-a3e4-a707b7cac420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "x, y = test_dataset[idx]\n",
    "output = model(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd70dc-50de-4bcf-8f95-8a4c0503ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1268b-3ee2-4167-a450-0068e6bfa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18232e0-e302-44e3-8ca9-6d59d02e91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a514-33e9-42c7-989d-96397ed4d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0402584-06d0-4746-aa54-bc87394577eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
