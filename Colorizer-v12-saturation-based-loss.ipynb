{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62340f-265b-4a2a-8032-c4d78cbbe551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from anything_vae import (\n",
    "    ResnetBlock2D,\n",
    "    SelfAttention,\n",
    "    Downsample2D,\n",
    "    Upsample2D,\n",
    "    DownEncoderBlock2D,\n",
    "    UpDecoderBlock2D,\n",
    "    UNetMidBlock2D,\n",
    "    Encoder,\n",
    "    Decoder,\n",
    "    # AutoencoderKL,\n",
    "    # VGGPerceptualLoss\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models as torchvision_models\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers, callbacks\n",
    "# import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501fa544-ca50-4173-8ac0-00d5f5023f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    # data\n",
    "    def __init__(self, data_folder, data_csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dir (string): Directory with all the input images.\n",
    "            output_dir (string): Directory with all the target (color) images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.data_path = os.path.join(data_folder, data_csv)\n",
    "        self.images = pd.read_csv(self.data_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB by replicating channels\n",
    "            transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "        ])\n",
    "        self.tranform_output = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        sketch_image = self.transform(self.__loadImage(sketch))\n",
    "        colored_image = self.tranform_output(self.__loadImage(colored))\n",
    "        return sketch_image, colored_image\n",
    "\n",
    "    def viewImage(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        return self.__loadImage(sketch), self.__loadImage(colored)\n",
    "\n",
    "    def __loadImage(self, image_path):\n",
    "        return Image.open(os.path.join(self.data_folder, image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c999b1-0fc7-438d-9062-0c945538f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(LightningModule):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg_model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.features = list(self.vgg.features[:16])\n",
    "        self.features = nn.Sequential(*self.features).eval()\n",
    "        \n",
    "        for params in self.features.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.criterion(self.features(x),self.features(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a371c0b5-5b32-4103-a6a2-c325760c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colorizer(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)  # Output 4 channels from quant_conv\n",
    "        self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)  # Expect 4 channels here\n",
    "        vgg_model = vgg16(weights=True)\n",
    "\n",
    "        self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.hparams.learning_rate = 0.00001\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)  # h: [batch_size, 8, H, W]\n",
    "\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)  # Each: [batch_size, 4, H, W]\n",
    "\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + std * eps  # z: [batch_size, 4, H, W]\n",
    "\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.post_quant_conv(z)  # z: [batch_size, 4, H, W]\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        perceptual_loss = self.loss_fn(x_recon, x)\n",
    "        mse_loss = self.mse_loss_fn(x_recon, x)\n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        \n",
    "        return x_recon\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.hparams.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "    \n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "        mse_loss = self.mse_loss_fn(outputs, targets)\n",
    "    \n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        self.log('train_loss', total_loss)\n",
    "        self.log('perceptual_loss', perceptual_loss)\n",
    "        self.log('mse_loss', mse_loss)\n",
    "    \n",
    "        # Log images every N batches\n",
    "        if batch_idx % 200 == 0:\n",
    "            num_images = 4  # Number of images to log\n",
    "            input_images = inputs[:num_images].detach().cpu()\n",
    "            target_images = targets[:num_images].detach().cpu()\n",
    "            output_images = outputs[:num_images].detach().cpu()\n",
    "    \n",
    "            # Concatenate images along the width (dim=3)\n",
    "            # Each image will be (C, H, W), so concatenating along dim=3 (width)\n",
    "            combined_images = torch.cat((input_images, target_images, output_images), dim=3)\n",
    "    \n",
    "            # Create a grid of images\n",
    "            grid = torchvision.utils.make_grid(combined_images, nrow=1)\n",
    "    \n",
    "            # Log the combined image grid to TensorBoard\n",
    "            self.logger.experiment.add_image('Input_Target_Output', grid, self.global_step)\n",
    "    \n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "    \n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "        mse_loss = self.mse_loss_fn(outputs, targets)\n",
    "    \n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        self.log('val_loss', total_loss)\n",
    "        self.log('val_perceptual_loss', perceptual_loss)\n",
    "        self.log('val_mse_loss', mse_loss)\n",
    "    \n",
    "        # Log images once per validation epoch\n",
    "        if batch_idx == 0:\n",
    "            num_images = 4\n",
    "            input_images = inputs[:num_images].detach().cpu()\n",
    "            target_images = targets[:num_images].detach().cpu()\n",
    "            output_images = outputs[:num_images].detach().cpu()\n",
    "    \n",
    "            # Concatenate images along width\n",
    "            combined_images = torch.cat((input_images, target_images, output_images), dim=3)\n",
    "    \n",
    "            # Create a grid of images\n",
    "            grid = torchvision.utils.make_grid(combined_images, nrow=1)\n",
    "    \n",
    "            # Log the combined image grid to TensorBoard\n",
    "            self.logger.experiment.add_image('Val_Input_Target_Output', grid, self.current_epoch)\n",
    "    \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b02076-c894-4cf9-bc6e-9199a07cee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "chkpt_file = '~/workspace/checkpoints/version_13.ckpt'\n",
    "model = Colorizer.load_from_checkpoint(chkpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ff49e4-8c48-44a2-a38a-4e4df10013ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrained_model = torch.load('anything-vae.pth', map_location='cpu')\n",
    "# model = Colorizer()\n",
    "# pretrained_state_dict = pretrained_model.state_dict()\n",
    "# missing_keys, unexpected_keys = model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "# filtered_missing_keys = [key for key in missing_keys if not key.startswith('loss_fn')]\n",
    "# assert len(filtered_missing_keys) == 0\n",
    "# assert len(unexpected_keys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63dc6bb-a4d1-403a-93b7-78cb980697d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/toy'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f9efaf-17f4-4895-819f-d2ae21cd618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = loggers.TensorBoardLogger(\"tb_logs\")\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=20, logger=logger, log_every_n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8ce4e5-a70b-4e74-ab36-3fa834cbb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder         | Encoder           | 34.2 M\n",
      "1 | decoder         | Decoder           | 49.5 M\n",
      "2 | quant_conv      | Conv2d            | 72    \n",
      "3 | post_quant_conv | Conv2d            | 20    \n",
      "4 | loss_fn         | VGGPerceptualLoss | 138 M \n",
      "5 | mse_loss_fn     | MSELoss           | 0     \n",
      "------------------------------------------------------\n",
      "220 M     Trainable params\n",
      "1.7 M     Non-trainable params\n",
      "222 M     Total params\n",
      "888.046   Total estimated model params size (MB)\n",
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 90/8199 [00:54<1:21:34,  1.66it/s, v_num=9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4f6991-ed37-4c6b-92bb-da9548e614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, training_dataset, device)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be137035-d800-49d2-aa32-a1b56fcc6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def viewTensor(output):\n",
    "    image = to_pil_image(output.squeeze())\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2af58b2-33bb-4b65-8644-48112b2b7020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Colorizer(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (attentions): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (attentions): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0-1): 2 x UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (loss_fn): VGGPerceptualLoss(\n",
       "    (vgg): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (25): ReLU(inplace=True)\n",
       "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (27): ReLU(inplace=True)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Dropout(p=0.5, inplace=False)\n",
       "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (mse_loss_fn): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "data_folder = 'data/test'\n",
    "data_csv = 'data.csv'\n",
    "test_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabfd6-8430-4466-a3e4-a707b7cac420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "x, y = test_dataset[idx]\n",
    "output = model(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd70dc-50de-4bcf-8f95-8a4c0503ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1268b-3ee2-4167-a450-0068e6bfa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18232e0-e302-44e3-8ca9-6d59d02e91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a514-33e9-42c7-989d-96397ed4d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
