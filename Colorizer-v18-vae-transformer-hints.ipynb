{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62340f-265b-4a2a-8032-c4d78cbbe551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from anything_vae import (\n",
    "    Encoder,\n",
    "    Decoder,\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models as torchvision_models\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers, callbacks\n",
    "# import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501fa544-ca50-4173-8ac0-00d5f5023f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    # data\n",
    "    def __init__(self, data_folder, data_csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dir (string): Directory with all the input images.\n",
    "            output_dir (string): Directory with all the target (color) images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.data_path = os.path.join(data_folder, data_csv)\n",
    "        self.images = pd.read_csv(self.data_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB by replicating channels\n",
    "            transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "        ])\n",
    "        self.tranform_output = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "        # return len(self.images)/\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        sketch_image = self.transform(self.__loadImage(sketch))\n",
    "        colored_image = self.tranform_output(self.__loadImage(colored))\n",
    "        return sketch_image, colored_image\n",
    "\n",
    "    def viewImage(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        return self.__loadImage(sketch), self.__loadImage(colored)\n",
    "\n",
    "    def __loadImage(self, image_path):\n",
    "        return Image.open(os.path.join(self.data_folder, image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c999b1-0fc7-438d-9062-0c945538f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(LightningModule):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg_model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.features = list(self.vgg.features[:16])\n",
    "        self.features = nn.Sequential(*self.features).eval()\n",
    "        \n",
    "        for params in self.features.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.criterion(self.features(x),self.features(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a371c0b5-5b32-4103-a6a2-c325760c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorHintEmbedding(nn.Module):\n",
    "    def __init__(self, n_colors=50, color_dim=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.color_embedding = nn.Linear(color_dim, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(n_colors, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, colors, positions):\n",
    "        # colors: [batch_size, n_colors, 3]\n",
    "        # positions: [batch_size, n_colors, 2] (x,y coordinates)\n",
    "        b, n, _ = colors.shape\n",
    "        color_embed = self.color_embedding(colors)  # [b, n, embed_dim]\n",
    "        pos_embed = self.position_embedding(positions)  # [b, n, embed_dim]\n",
    "        return self.norm(color_embed + pos_embed)\n",
    "\n",
    "class ReferenceImageEncoder(nn.Module):\n",
    "    def __init__(self, transformer_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, transformer_dim, 3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(8, transformer_dim)\n",
    "        \n",
    "    def forward(self, ref_image):\n",
    "        return self.norm(self.conv_blocks(ref_image))\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Conv2d(dim, dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # If context is provided, use it for key and value\n",
    "        if context is not None:\n",
    "            q = self.to_qkv(x)[:, :self.dim, :, :]  # Only get query from x\n",
    "            k = self.to_qkv(context)[:, self.dim:self.dim*2, :, :]  # Get key from context\n",
    "            v = self.to_qkv(context)[:, self.dim*2:, :, :]  # Get value from context\n",
    "        else:\n",
    "            qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "            q, k, v = qkv\n",
    "\n",
    "        q, k, v = map(lambda t: t.reshape(b, self.heads, -1, h * w), (q, k, v))\n",
    "\n",
    "        dots = torch.matmul(q.transpose(-2, -1), k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = torch.matmul(attn, v.transpose(-2, -1))\n",
    "        out = out.reshape(b, -1, h, w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(dim, heads)\n",
    "        self.color_cross_attn = SelfAttention(dim, heads)\n",
    "        self.ref_cross_attn = SelfAttention(dim, heads)\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(8, dim)\n",
    "        self.norm2 = nn.GroupNorm(8, dim)\n",
    "        self.norm_color = nn.GroupNorm(8, dim)\n",
    "        self.norm_ref = nn.GroupNorm(8, dim)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim * 4, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(dim * 4, dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, color_hints=None, ref_features=None):\n",
    "        # Self-attention\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        \n",
    "        # Cross-attention with color hints if provided\n",
    "        if color_hints is not None:\n",
    "            b, c, h, w = x.shape\n",
    "            # Reshape color hints to spatial dimension\n",
    "            color_hints = color_hints.view(b, c, -1).permute(0, 2, 1)\n",
    "            color_hints = color_hints.view(b, c, h, w)\n",
    "            x = x + self.color_cross_attn(self.norm_color(x), color_hints)\n",
    "            \n",
    "        # Cross-attention with reference image features if provided\n",
    "        if ref_features is not None:\n",
    "            x = x + self.ref_cross_attn(self.norm_ref(x), ref_features)\n",
    "        \n",
    "        # FFN\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Colorizer(LightningModule):\n",
    "    def __init__(self, checkpoint_path=None, transformer_dim=256, transformer_heads=8):\n",
    "        super(Colorizer, self).__init__()\n",
    "        \n",
    "        if checkpoint_path is not None:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            \n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            \n",
    "            self.encoder.load_state_dict(\n",
    "                {k.replace('encoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('encoder.')}\n",
    "            )\n",
    "            self.decoder.load_state_dict(\n",
    "                {k.replace('decoder.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('decoder.')}\n",
    "            )\n",
    "            self.quant_conv.load_state_dict(\n",
    "                {k.replace('quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('quant_conv.')}\n",
    "            )\n",
    "            self.post_quant_conv.load_state_dict(\n",
    "                {k.replace('post_quant_conv.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('post_quant_conv.')}\n",
    "            )\n",
    "            \n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            \n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.post_quant_conv.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "            print(\"Loaded pretrained weights from checkpoint\")\n",
    "        else:\n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)\n",
    "            self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)\n",
    "            vgg_model = vgg16(weights=True)\n",
    "            self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "            self.mse_loss_fn = nn.MSELoss()\n",
    "            print(\"Initialized new model from scratch\")\n",
    "        \n",
    "        # Initialize transformer and hint processing components\n",
    "        self.to_transformer_dim = nn.Conv2d(4, transformer_dim, 1)\n",
    "        self.transformer = TransformerBlock(transformer_dim, transformer_heads)\n",
    "        self.from_transformer_dim = nn.Conv2d(transformer_dim, 4, 1)\n",
    "        \n",
    "        # Add color hint and reference image processing\n",
    "        self.color_hint_processor = ColorHintEmbedding(\n",
    "            n_colors=50,\n",
    "            color_dim=3,\n",
    "            embed_dim=transformer_dim\n",
    "        )\n",
    "        self.ref_image_encoder = ReferenceImageEncoder(transformer_dim)\n",
    "        \n",
    "        # Training monitoring\n",
    "        self.num_high_loss_images = 50\n",
    "        self.high_loss_heap = []\n",
    "        self.current_min_high_loss = 0\n",
    "        \n",
    "        self.hparams.learning_rate = 0.0001\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + std * eps\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, color_hints=None, ref_image=None):\n",
    "        z = self.post_quant_conv(z)\n",
    "        z = self.to_transformer_dim(z)\n",
    "        \n",
    "        # Process hints if provided\n",
    "        color_features = None\n",
    "        ref_features = None\n",
    "        \n",
    "        if color_hints is not None:\n",
    "            colors, positions = color_hints\n",
    "            color_features = self.color_hint_processor(colors, positions)\n",
    "            \n",
    "        if ref_image is not None:\n",
    "            ref_features = self.ref_image_encoder(ref_image)\n",
    "        \n",
    "        # Apply transformer with hints\n",
    "        z = self.transformer(z, color_features, ref_features)\n",
    "        \n",
    "        z = self.from_transformer_dim(z)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x, color_hints=None, ref_image=None):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z, color_hints, ref_image)\n",
    "        return x_recon\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()), \n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack batch - now supports hints\n",
    "        if len(batch) == 2:\n",
    "            inputs, targets = batch\n",
    "            color_hints = None\n",
    "            ref_image = None\n",
    "        else:\n",
    "            inputs, targets, color_hints, ref_image = batch\n",
    "        \n",
    "        outputs = self(inputs, color_hints, ref_image)\n",
    "        \n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "        mse_loss = self.mse_loss_fn(outputs, targets)\n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        \n",
    "        # Store high loss images\n",
    "        self.store_high_loss_image(total_loss, inputs, targets, outputs)\n",
    "        \n",
    "        # Logging\n",
    "        self.log('train_loss', total_loss)\n",
    "        self.log('perceptual_loss', perceptual_loss)\n",
    "        self.log('mse_loss', mse_loss)\n",
    "        \n",
    "        # Visualization logic\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            self.visualize_high_loss_images(self.logger, self.global_step)\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            num_images = min(4, inputs.shape[0])\n",
    "            for i in range(num_images):\n",
    "                grid = self.visualize_single_output(\n",
    "                    inputs[i],\n",
    "                    outputs[i],\n",
    "                    targets[i]\n",
    "                )\n",
    "                self.logger.experiment.add_image(\n",
    "                    f'Sample_Images/sample_{i+1}',\n",
    "                    grid,\n",
    "                    self.global_step\n",
    "                )\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    # The rest of your methods (store_high_loss_image, visualize_model_output, etc.) remain unchanged\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()), \n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "\n",
    "    def store_high_loss_image(self, loss, inputs, targets, outputs):\n",
    "        \"\"\"\n",
    "        Store high loss images efficiently using a min heap to maintain top N\n",
    "        All tensors are stored in CPU memory\n",
    "        \"\"\"\n",
    "        # Convert to CPU and detach from computation graph\n",
    "        cpu_data = {\n",
    "            'loss': loss.item(),\n",
    "            'inputs': inputs.detach().cpu(),\n",
    "            'targets': targets.detach().cpu(),\n",
    "            'outputs': outputs.detach().cpu()\n",
    "        }\n",
    "        \n",
    "        if len(self.high_loss_heap) < self.num_high_loss_images:\n",
    "            # Heap not full yet, add new entry\n",
    "            heapq.heappush(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = min(loss.item(), self.current_min_high_loss if self.high_loss_heap else float('inf'))\n",
    "        elif loss.item() > self.current_min_high_loss:\n",
    "            # Remove lowest loss entry and add new higher loss entry\n",
    "            heapq.heapreplace(self.high_loss_heap, (loss.item(), cpu_data))\n",
    "            self.current_min_high_loss = self.high_loss_heap[0][0]\n",
    "\n",
    "\n",
    "    def visualize_single_output(self, input_img, output_img, target_img):\n",
    "        \"\"\"Helper function to create a single row grid for one set of images\"\"\"\n",
    "        # Ensure we're working with batched images\n",
    "        if input_img.dim() == 3:\n",
    "            input_img = input_img.unsqueeze(0)\n",
    "            output_img = output_img.unsqueeze(0)\n",
    "            target_img = target_img.unsqueeze(0)\n",
    "            \n",
    "        # Create row with input, output, and target\n",
    "        row = torch.cat([input_img, output_img, target_img], dim=0)\n",
    "        \n",
    "        # Handle grayscale images\n",
    "        if row.shape[1] == 1:\n",
    "            row = row.repeat(1, 3, 1, 1)\n",
    "            \n",
    "        # Create grid with the three images side by side\n",
    "        grid = torchvision.utils.make_grid(row, nrow=3, normalize=True, padding=2)\n",
    "        return grid\n",
    "\n",
    "    def visualize_high_loss_images(self, logger, step):\n",
    "        \"\"\"Visualize stored high loss images individually\"\"\"\n",
    "        if not self.high_loss_heap:\n",
    "            return\n",
    "            \n",
    "        # Sort by loss in descending order\n",
    "        sorted_entries = sorted(self.high_loss_heap, key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Log each high-loss image separately\n",
    "        for idx, (loss_value, data) in enumerate(sorted_entries):\n",
    "            grid = self.visualize_single_output(\n",
    "                data['inputs'],\n",
    "                data['outputs'],\n",
    "                data['targets']\n",
    "            )\n",
    "\n",
    "            logger.experiment.add_image('High_Loss_Images', grid, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b02076-c894-4cf9-bc6e-9199a07cee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights from checkpoint\n"
     ]
    }
   ],
   "source": [
    "chkpt_file = 'checkpoints/version_15.ckpt'\n",
    "model = Colorizer(checkpoint_path=chkpt_file, transformer_dim=256, transformer_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ff49e4-8c48-44a2-a38a-4e4df10013ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrained_model = torch.load('anything-vae.pth', map_location='cpu')\n",
    "# model = Colorizer()\n",
    "# pretrained_state_dict = pretrained_model.state_dict()\n",
    "# missing_keys, unexpected_keys = model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "# filtered_missing_keys = [key for key in missing_keys if not key.startswith('loss_fn')]\n",
    "# assert len(filtered_missing_keys) == 0\n",
    "# assert len(unexpected_keys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63dc6bb-a4d1-403a-93b7-78cb980697d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/toy'\n",
    "# data_folder = 'data/training'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f9efaf-17f4-4895-819f-d2ae21cd618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = loggers.TensorBoardLogger(\"tb_logs\", name='frozen-pretrained-vae-with-new-transformer')\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=20, logger=logger, log_every_n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce4e5-a70b-4e74-ab36-3fa834cbb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                 | Type                  | Params\n",
      "----------------------------------------------------------------\n",
      "0  | encoder              | Encoder               | 34.2 M\n",
      "1  | decoder              | Decoder               | 49.5 M\n",
      "2  | quant_conv           | Conv2d                | 72    \n",
      "3  | post_quant_conv      | Conv2d                | 20    \n",
      "4  | loss_fn              | VGGPerceptualLoss     | 138 M \n",
      "5  | mse_loss_fn          | MSELoss               | 0     \n",
      "6  | to_transformer_dim   | Conv2d                | 1.3 K \n",
      "7  | transformer          | TransformerBlock      | 1.3 M \n",
      "8  | from_transformer_dim | Conv2d                | 1.0 K \n",
      "9  | color_hint_processor | ColorHintEmbedding    | 14.3 K\n",
      "10 | ref_image_encoder    | ReferenceImageEncoder | 371 K \n",
      "----------------------------------------------------------------\n",
      "138 M     Trainable params\n",
      "85.4 M    Non-trainable params\n",
      "223 M     Total params\n",
      "894.857   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████▉| 998/1000 [10:02<00:01,  1.66it/s, v_num=18] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  50%|█████     | 501/1000 [05:15<05:14,  1.59it/s, v_num=18] "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f6991-ed37-4c6b-92bb-da9548e614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, training_dataset, device)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be137035-d800-49d2-aa32-a1b56fcc6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def viewTensor(output):\n",
    "    image = to_pil_image(output.squeeze())\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af58b2-33bb-4b65-8644-48112b2b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data_folder = 'data/test'\n",
    "data_csv = 'data.csv'\n",
    "test_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabfd6-8430-4466-a3e4-a707b7cac420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "x, y = test_dataset[idx]\n",
    "output = model(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd70dc-50de-4bcf-8f95-8a4c0503ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1268b-3ee2-4167-a450-0068e6bfa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18232e0-e302-44e3-8ca9-6d59d02e91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a514-33e9-42c7-989d-96397ed4d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
