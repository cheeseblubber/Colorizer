{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62340f-265b-4a2a-8032-c4d78cbbe551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from anything_vae import (\n",
    "    ResnetBlock2D,\n",
    "    SelfAttention,\n",
    "    Downsample2D,\n",
    "    Upsample2D,\n",
    "    DownEncoderBlock2D,\n",
    "    UpDecoderBlock2D,\n",
    "    UNetMidBlock2D,\n",
    "    Encoder,\n",
    "    Decoder,\n",
    "    # AutoencoderKL,\n",
    "    # VGGPerceptualLoss\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchvision import transforms, models as torchvision_models\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers, callbacks\n",
    "# import pytorch_lightning as pl\n",
    "from torchmetrics import MeanSquaredError\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501fa544-ca50-4173-8ac0-00d5f5023f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    # data\n",
    "    def __init__(self, data_folder, data_csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dir (string): Directory with all the input images.\n",
    "            output_dir (string): Directory with all the target (color) images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data_folder = data_folder\n",
    "        self.data_path = os.path.join(data_folder, data_csv)\n",
    "        self.images = pd.read_csv(self.data_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB by replicating channels\n",
    "            transforms.ToTensor()  # Convert images to PyTorch tensors\n",
    "        ])\n",
    "        self.tranform_output = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        sketch_image = self.transform(self.__loadImage(sketch))\n",
    "        colored_image = self.tranform_output(self.__loadImage(colored))\n",
    "        return sketch_image, colored_image\n",
    "\n",
    "    def viewImage(self, idx):\n",
    "        sketch, colored = self.images.iloc[idx]\n",
    "        return self.__loadImage(sketch), self.__loadImage(colored)\n",
    "\n",
    "    def __loadImage(self, image_path):\n",
    "        return Image.open(os.path.join(self.data_folder, image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c999b1-0fc7-438d-9062-0c945538f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(LightningModule):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg_model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.features = list(self.vgg.features[:16])\n",
    "        self.features = nn.Sequential(*self.features).eval()\n",
    "        \n",
    "        for params in self.features.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.criterion(self.features(x),self.features(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a371c0b5-5b32-4103-a6a2-c325760c3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import kornia  # For color space conversions\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "class Colorizer(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(Colorizer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.quant_conv = nn.Conv2d(8, 8, kernel_size=1)  # Output 8 channels from quant_conv\n",
    "        self.post_quant_conv = nn.Conv2d(4, 4, kernel_size=1)  # Expect 4 channels here\n",
    "        \n",
    "        vgg_model = vgg16(weights='DEFAULT')  # Adjust based on your torchvision version\n",
    "        self.loss_fn = VGGPerceptualLoss(vgg_model)\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.high_loss_images = []  # To store high-loss images every 100 steps\n",
    "\n",
    "        self.hparams.learning_rate = 0.0001\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)  # h: [batch_size, 8, H, W]\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)  # Each: [batch_size, 4, H, W]\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + std * eps  # z: [batch_size, 4, H, W]\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.post_quant_conv(z)  # z: [batch_size, 4, H, W]\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_recon = self.decode(self.encode(x))\n",
    "        return x_recon\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()), \n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Ensure outputs and targets are in [0, 1] range\n",
    "        outputs = torch.clamp(outputs, 0.0, 1.0)\n",
    "        targets = torch.clamp(targets, 0.0, 1.0)\n",
    "\n",
    "        # Convert outputs and targets from RGB to LAB\n",
    "        outputs_lab = kornia.color.rgb_to_lab(outputs)\n",
    "        targets_lab = kornia.color.rgb_to_lab(targets)\n",
    "\n",
    "        # Normalize LAB channels\n",
    "        outputs_lab[:, 0, :, :] /= 100.0     # L channel from [0, 100] to [0, 1]\n",
    "        outputs_lab[:, 1:, :, :] /= 128.0    # A and B channels from [-128, 127] to [-1, ~1]\n",
    "\n",
    "        targets_lab[:, 0, :, :] /= 100.0\n",
    "        targets_lab[:, 1:, :, :] /= 128.0\n",
    "\n",
    "        # Compute MSE loss in LAB space\n",
    "        mse_loss = self.mse_loss_fn(outputs_lab, targets_lab)\n",
    "\n",
    "        # Compute perceptual loss in RGB space\n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        self.log('train_loss', total_loss)\n",
    "        self.log('perceptual_loss', perceptual_loss)\n",
    "        self.log('mse_loss', mse_loss)\n",
    "\n",
    "        if total_loss > 0.7:\n",
    "            self.high_loss_images.append((total_loss, inputs, targets, outputs))\n",
    "\n",
    "        # Every 100 images, log the highest-loss image\n",
    "        if (batch_idx + 1) % 100 == 0 and self.high_loss_images:\n",
    "            # Sort by loss and get the highest-loss image\n",
    "            high_loss_image = max(self.high_loss_images, key=lambda x: x[0])\n",
    "            _, input_img, target_img, output_img = high_loss_image\n",
    "\n",
    "            # Prepare images for logging\n",
    "            combined_image = torch.cat((input_img[0], target_img[0], output_img[0]), dim=2)\n",
    "            grid = torchvision.utils.make_grid(combined_image, nrow=1)\n",
    "            self.logger.experiment.add_image('High_Loss_Image', grid, self.global_step)\n",
    "\n",
    "            # Clear high-loss images list for the next 100 batches\n",
    "            self.high_loss_images.clear()\n",
    "\n",
    "        \n",
    "        # Log images every N batches\n",
    "        if batch_idx % 1000 == 0:\n",
    "            num_images = 4\n",
    "            input_images = inputs[:num_images].detach().cpu()\n",
    "            target_images = targets[:num_images].detach().cpu()\n",
    "            output_images = outputs[:num_images].detach().cpu()\n",
    "\n",
    "            # Concatenate images along width (dim=3)\n",
    "            combined_images = torch.cat((input_images, target_images, output_images), dim=3)\n",
    "            grid = torchvision.utils.make_grid(combined_images, nrow=1)\n",
    "\n",
    "            # Log the combined image grid to TensorBoard\n",
    "            self.logger.experiment.add_image('Input_Target_Output', grid, self.global_step)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Ensure outputs and targets are in [0, 1] range\n",
    "        outputs = torch.clamp(outputs, 0.0, 1.0)\n",
    "        targets = torch.clamp(targets, 0.0, 1.0)\n",
    "\n",
    "        # Convert outputs and targets from RGB to LAB\n",
    "        outputs_lab = kornia.color.rgb_to_lab(outputs)\n",
    "        targets_lab = kornia.color.rgb_to_lab(targets)\n",
    "\n",
    "        # Normalize LAB channels\n",
    "        outputs_lab[:, 0, :, :] /= 100.0     # L channel from [0, 100] to [0, 1]\n",
    "        outputs_lab[:, 1:, :, :] /= 128.0    # A and B channels from [-128, 127] to [-1, ~1]\n",
    "\n",
    "        targets_lab[:, 0, :, :] /= 100.0\n",
    "        targets_lab[:, 1:, :, :] /= 128.0\n",
    "\n",
    "        # Compute MSE loss in LAB space\n",
    "        mse_loss = self.mse_loss_fn(outputs_lab, targets_lab)\n",
    "\n",
    "        # Compute perceptual loss in RGB space\n",
    "        perceptual_loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "        total_loss = perceptual_loss + mse_loss\n",
    "        self.log('val_loss', total_loss)\n",
    "        self.log('val_perceptual_loss', perceptual_loss)\n",
    "        self.log('val_mse_loss', mse_loss)\n",
    "\n",
    "        # Log images once per validation epoch\n",
    "        if batch_idx == 0:\n",
    "            num_images = 4\n",
    "            input_images = inputs[:num_images].detach().cpu()\n",
    "            target_images = targets[:num_images].detach().cpu()\n",
    "            output_images = outputs[:num_images].detach().cpu()\n",
    "\n",
    "            # Concatenate images along width\n",
    "            combined_images = torch.cat((input_images, target_images, output_images), dim=3)\n",
    "            grid = torchvision.utils.make_grid(combined_images, nrow=1)\n",
    "\n",
    "            # Log the combined image grid to TensorBoard\n",
    "            self.logger.experiment.add_image('Val_Input_Target_Output', grid, self.current_epoch)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b02076-c894-4cf9-bc6e-9199a07cee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpt_file = '~/workspace/checkpoints/version_14.ckpt'\n",
    "# model = Colorizer.load_from_checkpoint(chkpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce7dd3f-73c4-4280-9142-8579399c8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Colorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ff49e4-8c48-44a2-a38a-4e4df10013ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pretrained_model = torch.load('anything-vae.pth', map_location='cpu')\n",
    "# model = Colorizer()\n",
    "# pretrained_state_dict = pretrained_model.state_dict()\n",
    "# missing_keys, unexpected_keys = model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "# filtered_missing_keys = [key for key in missing_keys if not key.startswith('loss_fn')]\n",
    "# assert len(filtered_missing_keys) == 0\n",
    "# assert len(unexpected_keys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63dc6bb-a4d1-403a-93b7-78cb980697d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/training'\n",
    "data_csv = 'data.csv'\n",
    "training_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f9efaf-17f4-4895-819f-d2ae21cd618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = loggers.TensorBoardLogger(\"tb_logs\")\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, max_epochs=5, logger=logger, log_every_n_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ce4e5-a70b-4e74-ab36-3fa834cbb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder         | Encoder           | 34.2 M\n",
      "1 | decoder         | Decoder           | 49.5 M\n",
      "2 | quant_conv      | Conv2d            | 72    \n",
      "3 | post_quant_conv | Conv2d            | 20    \n",
      "4 | loss_fn         | VGGPerceptualLoss | 138 M \n",
      "5 | mse_loss_fn     | MSELoss           | 0     \n",
      "------------------------------------------------------\n",
      "220 M     Trainable params\n",
      "1.7 M     Non-trainable params\n",
      "222 M     Total params\n",
      "888.046   Total estimated model params size (MB)\n",
      "/home/ubuntu/miniconda3/envs/dl-env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|███▉                                                                                                                                                | 3418/129629 [32:02<19:42:51,  1.78it/s, v_num=18]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|▋         | 9260/129629 [1:26:59<18:50:41,  1.77it/s, v_num=18]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  13%|█▎        | 16906/129629 [2:38:57<17:39:52,  1.77it/s, v_num=18]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f6991-ed37-4c6b-92bb-da9548e614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, training_dataset, device)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be137035-d800-49d2-aa32-a1b56fcc6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def viewTensor(output):\n",
    "    image = to_pil_image(output.squeeze())\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af58b2-33bb-4b65-8644-48112b2b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data_folder = 'data/test'\n",
    "data_csv = 'data.csv'\n",
    "test_dataset = ColorizationDataset(data_folder, data_csv)\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fabfd6-8430-4466-a3e4-a707b7cac420",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "x, y = test_dataset[idx]\n",
    "output = model(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd70dc-50de-4bcf-8f95-8a4c0503ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1268b-3ee2-4167-a450-0068e6bfa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18232e0-e302-44e3-8ca9-6d59d02e91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1a514-33e9-42c7-989d-96397ed4d88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
